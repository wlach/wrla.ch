<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
 <title type="text">William Lachance's Log: Posts tagged 'FirefoxOS'</title>
 <link rel="self" href="https://wlach.github.io/feeds/FirefoxOS.atom.xml" />
 <link href="https://wlach.github.io/tags/FirefoxOS.html" />
 <id>urn:https-wlach-github-io:-tags-FirefoxOS-html</id>
 <updated>2015-07-14T04:00:00Z</updated>
 <entry>
  <title type="text">Perfherder update</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2015/07/perfherder-update?utm_source=FirefoxOS&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2015-07-perfherder-update</id>
  <published>2015-07-14T04:00:00Z</published>
  <updated>2015-07-14T04:00:00Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;Haven&amp;#8217;t been doing enough blogging about &lt;a href="https://wiki.mozilla.org/Auto-tools/Projects/Perfherder"&gt;Perfherder&lt;/a&gt; (our project to make Talos and other per-checkin performance data more useful) recently. Let&amp;#8217;s fix that. We&amp;#8217;ve been making some good progress, helped in part by a group of new contributors that joined us through an experimental &amp;#8220;&lt;a href="https://elvis314.wordpress.com/2015/06/09/please-welcome-the-dashboard-hacker-team/"&gt;summer of contribution&lt;/a&gt;&amp;#8221; program.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comparison mode&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Inspired by Compare Talos, we&amp;#8217;ve designed something similar which hooks into the perfherder backend. This has already gotten some interest: see this post on &lt;a href="https://groups.google.com/d/msg/mozilla.dev.tree-management/IUmMuY8b52A/Asne1cW0I8EJ"&gt;dev.tree-management&lt;/a&gt; and this one on &lt;a href="https://groups.google.com/d/msg/mozilla.dev.platform/PaJFBtvc3Vg/BvX-pFlsAkoJ"&gt;dev.platform&lt;/a&gt;. We&amp;#8217;re working towards building something that will be really useful both for (1) illustrating that the performance regressions we detect are real and (2) helping developers figure out the impact of their changes before they land them.&lt;/p&gt;

&lt;table&gt;
 &lt;tr&gt;
  &lt;td&gt;&lt;a href="/files/2015/07/Screen-Shot-2015-07-14-at-3.54.57-PM.png"&gt;&lt;img src="/files/2015/07/Screen-Shot-2015-07-14-at-3.54.57-PM-300x207.png" alt="Screen Shot 2015-07-14 at 3.54.57 PM" width="300" height="207" class="alignnone size-medium wp-image-1219" srcset="/files/2015/07/Screen-Shot-2015-07-14-at-3.54.57-PM-300x207.png 300w, /files/2015/07/Screen-Shot-2015-07-14-at-3.54.57-PM.png 980w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;td&gt;&lt;a href="/files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM.png"&gt;&lt;img src="/files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM-300x206.png" alt="Screen Shot 2015-07-14 at 3.53.20 PM" width="300" height="206" class="alignnone size-medium wp-image-1218" srcset="/files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM-300x206.png 300w, /files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM.png 980w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Most of the initial work was done by &lt;a href="https://elvis314.wordpress.com/"&gt;Joel Maher&lt;/a&gt; with lots of review for aesthetics and correctness by me. Avi Halmachi from the Performance Team also helped out with the &lt;a href="https://en.wikipedia.org/wiki/Student's_t-test"&gt;t-test&lt;/a&gt; model for detecting the confidence that we have that a difference in performance was real. Lately myself and &lt;a href="https://github.com/MikeLing"&gt;Mike Ling&lt;/a&gt; (one of our summer of contribution members) have been working on further improving the interface for usability &amp;#8212; I&amp;#8217;m hopeful that we&amp;#8217;ll soon have something implemented that&amp;#8217;s broadly usable and comprehensible to the Mozilla Firefox and Platform developer community.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Graphs improvements&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Although it&amp;#8217;s received slightly less attention lately than the comparison view above, we&amp;#8217;ve been making steady progress on the graphs view of performance series. Aside from demonstrations and presentations, the primary use case for this is being able to detect visually sustained changes in the result distribution for talos tests, which is often necessary to be able to confirm regressions. Notable recent changes include a much easier way of selecting tests to add to the graph from Mike Ling and more readable/parseable urls from &lt;a href="https://github.com/akhileshpillai"&gt;Akhilesh Pillai&lt;/a&gt; (another summer of contribution participant).&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/07/Screen-Shot-2015-07-14-at-4.09.45-PM.png"&gt;&lt;img src="/files/2015/07/Screen-Shot-2015-07-14-at-4.09.45-PM-300x174.png" alt="Screen Shot 2015-07-14 at 4.09.45 PM" width="300" height="174" class="alignnone size-medium wp-image-1221" srcset="/files/2015/07/Screen-Shot-2015-07-14-at-4.09.45-PM-300x174.png 300w, /files/2015/07/Screen-Shot-2015-07-14-at-4.09.45-PM-1024x595.png 1024w, /files/2015/07/Screen-Shot-2015-07-14-at-4.09.45-PM.png 1130w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Performance alerts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ve also been steadily working on making Perfherder generate alerts when there is a significant discontinuity in the performance numbers, similar to what &lt;a href="http://graphs.mozilla.org"&gt;GraphServer&lt;/a&gt; does now. Currently we have an option to generate a static CSV file of these alerts, but the eventual plan is to insert these things into a peristent database. After that&amp;#8217;s done, we can actually work on creating a UI inside Perfherder to replace &lt;a href="http://alertmanager.allizom.org:8080/alerts.html#"&gt;alertmanager&lt;/a&gt; (which currently uses GraphServer data) and start using this thing to sheriff performance regressions &amp;#8212; putting the herder into perfherder.&lt;/p&gt;

&lt;p&gt;As part of this, I&amp;#8217;ve converted the graphserver alert generation code into a standalone python library, which has already proven useful as a component in the &lt;a href="https://hacks.mozilla.org/2015/06/performance-testing-firefox-os-with-raptor/"&gt;Raptor project for FirefoxOS&lt;/a&gt;. Yay modularity and reusability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Python API&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ve also been working on creating and improving a &lt;a href="http://treeherder.readthedocs.org/retrieving_data.html#python-client"&gt;python API&lt;/a&gt; to access Treeherder data, which includes Perfherder. This lets you do interesting things, like dynamically run various types of statistical analysis on the data stored in the production instance of Perfherder (no need to ask me for a database dump or other credentials). I&amp;#8217;ve been using this to perform validation of the data we&amp;#8217;re storing and debug various tricky problems. For example, &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1182282"&gt;I found out last week that we were storing up to duplicate 200 entries in each performance series due to double data ingestion&lt;/a&gt; &amp;#8212; oops.&lt;/p&gt;

&lt;p&gt;You can also use this API to dynamically create interesting graphs and visualizations using &lt;a href="http://wrla.ch/blog/2014/04/pycon-2014-impressions-ipython-notebook-is-the-future-more/"&gt;ipython notebook&lt;/a&gt;, here&amp;#8217;s a simple example of me plotting the last 7 days of youtube.com pageload data inline in a notebook:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/07/Screen-Shot-2015-07-14-at-4.43.55-PM.png"&gt;&lt;img src="/files/2015/07/Screen-Shot-2015-07-14-at-4.43.55-PM-300x224.png" alt="Screen Shot 2015-07-14 at 4.43.55 PM" width="300" height="224" class="alignnone size-medium wp-image-1224" srcset="/files/2015/07/Screen-Shot-2015-07-14-at-4.43.55-PM-300x224.png 300w, /files/2015/07/Screen-Shot-2015-07-14-at-4.43.55-PM.png 842w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[&lt;a href="http://nbviewer.ipython.org/url/wrla.ch/blog/wp-content/uploads/2015/07/perfherder-api.ipynb"&gt;original&lt;/a&gt;]&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">A new meditation app</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2014/08/a-new-meditation-app?utm_source=FirefoxOS&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2014-08-a-new-meditation-app</id>
  <published>2014-08-14T04:00:00Z</published>
  <updated>2014-08-14T04:00:00Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;I had some time on my hands two weekends ago and was feeling a bit of an itch to build something, so I decided to do a project I&amp;#8217;ve had in the back of my head for a while: a meditation timer.&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;ve been following this log, you&amp;#8217;d know that &lt;a href="http://wrla.ch/blog/category/meditation/"&gt;meditation&lt;/a&gt; has been a pretty major interest of mine for the past year. The foundation of my practice is a daily round of seated meditation at home, where I have been attempting to follow the breath and generally try to connect with the world for a set period every day (usually varying between 10 and 30 minutes, depending on how much of a rush I&amp;#8217;m in).&lt;/p&gt;

&lt;p&gt;Clock watching is rather distracting while sitting so having a tool to notify you when a certain amount of time has elapsed is quite useful. Writing a smartphone app to do this is an obvious idea, and indeed approximately a zillion of these things have been written for Android and iOS. Unfortunately, most are not very good. Really, I just want something that does this:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Select a meditation length (somewhere between 10 and 40 minutes).&lt;/li&gt;
 &lt;li&gt;Sound a bell after a short preparation to demarcate the beginning of meditation.&lt;/li&gt;
 &lt;li&gt;While the meditation period is ongoing, do a countdown of the time remaining (not strictly required, but useful for peace of mind in case you&amp;#8217;re wondering whether you&amp;#8217;ve really only sat for 25 minutes).&lt;/li&gt;
 &lt;li&gt;Sound a bell when the meditation ends.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;Yes, meditation can get more complex than that. In Zen practice, for example, sometimes you have several periods of varying length, broken up with kinhin (walking meditation). However, that mostly happens in the context of a formal setting (e.g. &lt;a href="http://en.wikipedia.org/wiki/Zendo"&gt;a Zendo&lt;/a&gt;) where you leave your smartphone at the door. Trying to shoehorn all that into an app needlessly complicates what should be simple.&lt;/p&gt;

&lt;p&gt;Even worse are the apps which &amp;#8220;chart&amp;#8221; your progress or have other gimmicks to connect you to a virtual &amp;#8220;community&amp;#8221; of meditators. I have to say I find that kind of stuff really turns me off. Meditation should be about connecting with reality in a more fundamental way, not charting gamified statistics or interacting online. We already have way too much of that going on elsewhere in our lives without adding even more to it.&lt;/p&gt;

&lt;p&gt;So, you might ask why the alarm feature of most clock apps isn&amp;#8217;t sufficient? Really, it is most of the time. A specialized app can make selecting the interval slightly more convenient and we can preselect an appropriate bell sound up front. It&amp;#8217;s also nice to hear something to demarcate the start of a meditation session. But honestly I didn&amp;#8217;t have much of a reason to write this other than the fact than I could. Outside of work, I&amp;#8217;ve been in a bit of a creative rut lately and felt like I needed to build something, anything and put it out into the world (even if it&amp;#8217;s tiny and only a very incremental improvement over what&amp;#8217;s out there already). So here it is:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/08/meditation-timer-screen.png"&gt;&lt;img src="/files/2014/08/meditation-timer-screen.png" alt="meditation-timer-screen" width="320" height="483" class="alignnone size-full wp-image-1089" srcset="/files/2014/08/meditation-timer-screen-198x300.png 198w, /files/2014/08/meditation-timer-screen.png 320w" sizes="(max-width: 320px) 100vw, 320px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The app was written entirely in HTML5 so it should work fine on pretty much any reasonably modern device, desktop or mobile. I tested it on my Nexus 5 (Chrome, Firefox for Android)&lt;sup&gt;&lt;a href="http://wrla.ch/blog/category/meditation/"&gt;1&lt;/a&gt;&lt;/sup&gt;, FirefoxOS Flame, and on my laptop (Chrome, Firefox, Safari). It lives on a &lt;a href="http://meditation.wrla.ch"&gt;subdomain of this site&lt;/a&gt; or you can &lt;a href="https://marketplace.firefox.com/app/meditation/"&gt;grab it from the Firefox Marketplace&lt;/a&gt; if you&amp;#8217;re using some variant of Firefox (OS). The source, such as it is, &lt;a href="http://github.com/wlach/meditation"&gt;can be found&lt;/a&gt; on github.&lt;/p&gt;

&lt;p&gt;I should acknowledge taking some design inspiration from the &lt;a href="http://helloform.com/projects/mind/"&gt;Mind application&lt;/a&gt; for iOS, which has a similarly minimalistic take on things. Check that out too if you have an iPhone or iPad!&lt;/p&gt;

&lt;p&gt;Happy meditating!&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;&lt;a href="http://wrla.ch/blog/category/meditation/"&gt;1&lt;/a&gt;&lt;/sup&gt; Note that there isn&amp;#8217;t a way to inhibit the screen/device from going to sleep with these browsers, which means that you might miss the ending bell. On FirefoxOS, I used the &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Navigator.requestWakeLock"&gt;requestWakeLock&lt;/a&gt; API to make sure that doesn&amp;#8217;t happen. I filed &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1054113"&gt;a bug&lt;/a&gt; to get this implemented on Firefox for Android.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Measuring frames per second and animation smoothness with Eideticker</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2014/07/measuring-frames-per-second-and-animation-smoothness-with-eideticker?utm_source=FirefoxOS&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2014-07-measuring-frames-per-second-and-animation-smoothness-with-eideticker</id>
  <published>2014-07-07T04:00:00Z</published>
  <updated>2014-07-07T04:00:00Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Just wanted to write up a few notes on using Eideticker to measure animation smoothness, since this is a topic that comes up pretty often and I wind up explaining these things repeatedly. ðŸ˜‰&lt;/p&gt;

&lt;p&gt;When rendering web content, we want the screen to update something like 60 times per second (typical refresh rate of an LCD screen) when an animation or other change is occurring. When this isn&amp;#8217;t happening, there is often a user perception of jank (a.k.a. things not working as they should). Generally we express how well we measure up to this ideal by counting the number of &amp;#8220;frames per second&amp;#8221; that we&amp;#8217;re producing. If you&amp;#8217;re reading this, you&amp;#8217;re probably already familiar with the concept in outline. If you want to know more, you can check out the &lt;a href="http://en.wikipedia.org/wiki/Frame_rate"&gt;wikipedia article&lt;/a&gt; which goes into more detail.&lt;/p&gt;

&lt;p&gt;At an internal level, this concept matches up conceptually with what Gecko is doing. The graphics pipeline produces frames inside graphics memory, which is then sent to the LCD display (whether it be connected to a laptop or a mobile phone) to be viewed. By instrumenting the code, we can see how often this is happening, and whether it is occurring at the right frequency to reach 60 fps. My understanding is that we have at least some code which does exactly this, though I&amp;#8217;m not 100% up to date on how accurate it is.&lt;/p&gt;

&lt;p&gt;But even assuming the best internal system monitoring, Eideticker might still be useful because:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;It is more &amp;#8220;objective&amp;#8221;. This is valuable not only for our internal purposes to validate other automation (sometimes internal instrumentation can be off due to a bug or whatever), but also to &amp;#8220;prove&amp;#8221; to partners that our software has the performance characteristics that we claim.&lt;/li&gt;
 &lt;li&gt;The visual artifacts it leaves behind can be valuable for inspection and debugging. i.e. &lt;a href="http://wrla.ch/blog/2012/09/more-eideticker-happenings-profiling-and-startup-testing/"&gt;you can correlate videos with profiling information&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Unfortunately, deriving this sort of information from a video capture is more complicated than you&amp;#8217;d expect.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What does frames per second even mean?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given a set of N frames captured from the device, the immediate solution when it comes to &amp;#8220;frames per second&amp;#8221; is to just compare frames against each other (e.g. by comparing the value of individual pixels) and then counting the ones that are different as &amp;#8220;unique frames&amp;#8221;. Divide the total number of unique frames by the length of the
 &lt;br /&gt;capture and&amp;#8230; voila? Frames per second? Not quite.&lt;/p&gt;

&lt;p&gt;First off, there&amp;#8217;s the inherent problem that sometimes the expected behaviour of a test is for the screen to be unchanging for a period of time. For example, at the very beginning of a capture (when we are waiting for the input event to be acknowledged) and at the end (when we are waiting for things to settle). Second, it&amp;#8217;s also easy to imagine the display remaining static for a period of time in the middle of a capture (say in between gestures in a multi-part capture). In these cases, there will likely be no observable change on the screen and thus the number of frames counted will be artificially low, skewing the frames per second number down.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Measurement problems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ok, so you might not consider that class of problem that big a deal. Maybe we could just not consider the frames at the beginning or end of the capture. And for pauses in the middle&amp;#8230; as long as we get an absolute number at the end, we&amp;#8217;re fine right? That&amp;#8217;s at least enough to let us know that we&amp;#8217;re getting better or worse, assuming that whatever we&amp;#8217;re testing is behaving the same way between runs and we&amp;#8217;re just trying to measure how many frames hit the screen.&lt;/p&gt;

&lt;p&gt;I might agree with you there, but there&amp;#8217;s a further problems that are specific to measuring on-screen performance using a high-speed camera as we are currently with FirefoxOS.&lt;/p&gt;

&lt;p&gt;An LCD updates gradually, and not all at once. Remnants of previous frames will remain on screen long past their interval. Take for example these five frames (sampled at 120fps) from a capture of a pan down in the FirefoxOS Contacts application (&lt;a href="/files/2014/07/movie.webm"&gt;movie&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/07/sidebyside.png"&gt;&lt;img src="/files/2014/07/sidebyside-1024x263.png" alt="sidebyside" width="474" height="121" class="alignnone size-large wp-image-1074" srcset="/files/2014/07/sidebyside-300x77.png 300w, /files/2014/07/sidebyside-1024x263.png 1024w" sizes="(max-width: 474px) 100vw, 474px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note how if you look closely these 5 frames are actually the intersection of *three* seperate frames. One with &amp;#8220;Adam Card&amp;#8221; at the top, another with &amp;#8220;Barbara Bloomquist&amp;#8221; at the top, then another with &amp;#8220;Barbara Bloomquist&amp;#8221; even further up. Between each frame, artifacts of the previous one are clearly visible.&lt;/p&gt;

&lt;p&gt;Plausible sounding solutions:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Try to resolve the original images by distinguishing &amp;#8220;new&amp;#8221; content from ghosting artifacts. Sounds possible, but probably hard? I&amp;#8217;ve tried a number of simplistic techniques (i.e. trying to find times when change is &amp;#8220;peaking&amp;#8221;), but nothing has really worked out very well.&lt;/li&gt;
 &lt;li&gt;Somehow reverse engineering the interface between the graphics chipset and the LCD panel, and writing some kind of custom hardware to &amp;#8220;capture&amp;#8221; the framebuffer as it is being sent from one to the other. Also sounds difficult.&lt;/li&gt;
 &lt;li&gt;Just forget about this problem altogether and only try to capture periods of time in the capture where the image has stayed static for a sustained period of time (i.e. for say 4&amp;ndash;5 frames and up) and we&amp;#8217;re pretty sure it&amp;#8217;s jank.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Personally the last solution appeals to me the most, although it has the obvious disadvantage of being a &amp;#8220;homebrew&amp;#8221; metric that no one has ever heard of before, which might make it difficult to use to prove that performance is adequate &amp;#8212; the numbers come with a long-winded explanation instead of being something that people immediately understand.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">End of Q2 Eideticker update: Flame tests, future plans</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2014/06/end-of-q2-eideticker-update-flame-tests-future-plans?utm_source=FirefoxOS&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2014-06-end-of-q2-eideticker-update-flame-tests-future-plans</id>
  <published>2014-06-27T04:00:00Z</published>
  <updated>2014-06-27T04:00:00Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Just wanted to give an update on where Eideticker is at the end of Q2 2014. The big news is that we&amp;#8217;ve started to run startup tests against the Flame, the results of which are starting to appear on the dashboard:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/06/eideticker-contacts-flame.png"&gt;&lt;img src="/files/2014/06/eideticker-contacts-flame.png" alt="eideticker-contacts-flame" width="1002" height="664" class="alignnone size-full wp-image-1062" srcset="/files/2014/06/eideticker-contacts-flame-300x198.png 300w, /files/2014/06/eideticker-contacts-flame.png 1002w" sizes="(max-width: 1002px) 100vw, 1002px" /&gt;&lt;/a&gt; &lt;a href="http://eideticker.mozilla.org/b2g/#/flame/b2g-contacts-startup/timetostableframe"&gt;[link]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It is expected that these tests will provide a useful complement to the &lt;a href="https://datazilla.mozilla.org/b2g/?branch=master&amp;amp;#038;device=flame&amp;amp;#038;range=7&amp;amp;#038;test=cold_load_time&amp;amp;#038;app_list=browser,calendar,camera,clock,contacts,email%20FTU,fm_radio,gallery,marketplace,messages,music,phone,settings,template,usage,video&amp;amp;#038;app=phone&amp;amp;#038;gaia_rev=b8f36518696f3191&amp;amp;#038;gecko_rev=c90b38c47a1d&amp;amp;#038;plot=avg"&gt;existing startup tests&lt;/a&gt; we&amp;#8217;re running with b2gperf, in particular answering the &amp;#8220;is this regression real?&amp;#8221; question.&lt;/p&gt;

&lt;p&gt;Pending work for Q3:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Enable scrolling tests on the Flame. I got these working against the Hamachi &lt;a href="http://wrla.ch/blog/2014/03/its-all-about-the-entropy/"&gt;a few months ago&lt;/a&gt; but because of some weird input issue we&amp;#8217;re seeing we can&amp;#8217;t yet enable them on the Flame. This is being tracked in &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1028824"&gt;bug 1028824&lt;/a&gt;. If anyone has background on the behaviour of the touch screen driver for this device I would appreciate some help.&lt;/li&gt;
 &lt;li&gt;Enable tests for multiple branches on the Flame (currently we&amp;#8217;re only doing master). This is pretty much ready to go (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1017834"&gt;bug 1017834&lt;/a&gt;), just need to land it.&lt;/li&gt;
 &lt;li&gt;Annotate eideticker graphs with internal benchmark information. Eli Perelman of the FirefoxOS performance team has come up with a standard set of on-load events for the Gaia apps (app chrome loaded, app content loaded, &amp;#8230;) that each app will generate, feeding into tools like b2gperf and test-perf. We want to show this information in Eideticker&amp;#8217;s frame-by-frame analysis (&lt;a href="http://eideticker.mozilla.org/b2g/detail.html?id=2b007f8cfd8b11e3923c10ddb19eacac#/framecannyentropies"&gt;example&lt;/a&gt;) so we can verify that the app&amp;#8217;s behaviour is consistent with what it is claimed. This is being tracked in &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1018334"&gt;bug 1018334&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;Re-enable Eideticker for Android and run tests more frequently. Sadly we haven&amp;#8217;t been consistently generating new Eideticker results for Android for the last quarter because of networking issues in the new Mountain View office, where the test rig for those live. One way or another, we want to fix this next quarter and hopefully run tests more frequently against mozilla-inbound (instead of just nightly builds)&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;The above isn&amp;#8217;t an exhaustive list: there&amp;#8217;s much more that we have in mind for the future that&amp;#8217;s not yet scheduled or defined well (e.g. get Eideticker reporting to Treeherder&amp;#8217;s new performance module). If you have any questions or feedback on anything outlined above I&amp;#8217;d love to hear it!&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">It&amp;#8217;s all about the entropy</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2014/03/it-8217-s-all-about-the-entropy?utm_source=FirefoxOS&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2014-03-it-8217-s-all-about-the-entropy</id>
  <published>2014-03-14T04:00:00Z</published>
  <updated>2014-03-14T04:00:00Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So recently I&amp;#8217;ve been exploring new and different methods of measuring things that we care about on FirefoxOS &amp;#8212; like startup time or amount of &lt;a href="http://www.masonchang.com/blog/2014/3/2/wow-such-checkerboard"&gt;checkerboarding&lt;/a&gt;. With Android, where we have a mostly clean signal, these measurements were pretty straightforward. Want to measure startup times? Just capture a video of Firefox starting, then compare the frames pixel by pixel to see how much they differ. When the pixels aren&amp;#8217;t that different anymore, we&amp;#8217;re &amp;#8220;done&amp;#8221;. Likewise, to measure checkerboarding we just calculated the areas of the screen where things were not completely drawn yet, frame-by-frame.&lt;/p&gt;

&lt;p&gt;On FirefoxOS, where we&amp;#8217;re using a camera to measure these things, it has not been so simple. I&amp;#8217;ve already discussed this with respect to startup time in a &lt;a href="http://wrla.ch/blog/2013/10/automatically-measuring-startup-load-time-with-eideticker/"&gt;previous post&lt;/a&gt;. One of the ideas I talk about there is &amp;#8220;entropy&amp;#8221; (or the amount of unique information in the frame). It turns out that this is a pretty deep concept, and is useful for even more things than I thought of at the time. Since this is probably a concept that people are going to be thinking/talking about for a while, it&amp;#8217;s worth going into a little more detail about the math behind it.&lt;/p&gt;

&lt;p&gt;The &lt;a href="http://en.wikipedia.org/wiki/Shannon_entropy"&gt;wikipedia article&lt;/a&gt; on information theoretic entropy is a pretty good introduction. You should read it. It all boils down to this formula:&lt;/p&gt;

&lt;p&gt;&lt;img src="/files/2014/03/wikipedia-entropy-formula.png" alt="wikipedia-entropy-formula" width="401" height="37" class="alignnone size-full wp-image-1014" srcset="/files/2014/03/wikipedia-entropy-formula-300x27.png 300w, /files/2014/03/wikipedia-entropy-formula.png 401w" sizes="(max-width: 401px) 100vw, 401px" /&gt;&lt;/p&gt;

&lt;p&gt;You can see this section of the wikipedia article (and the various articles that it links to) if you want to break down where that comes from, but the short answer is that given a set of random samples, the more different values there are, the higher the entropy will be. Look at it from a probabilistic point of view: if you take a random set of data and want to make predictions on what future data will look like. If it is highly random, it will be harder to predict what comes next. Conversely, if it is more uniform it is easier to predict what form it will take.&lt;/p&gt;

&lt;p&gt;Another, possibly more accessible way of thinking about the entropy of a given set of data would be &amp;#8220;how well would it compress?&amp;#8221;. For example, a bitmap image with nothing but black in it could compress very well as there&amp;#8217;s essentially only 1 piece of unique information in it repeated many times &amp;#8212; the black pixel. On the other hand, a bitmap image of completely randomly generated pixels would probably compress very badly, as almost every pixel represents several dimensions of unique information. For all the statistics terminology, etc. that&amp;#8217;s all the above formula is trying to say.&lt;/p&gt;

&lt;p&gt;So we have a model of entropy, now what? For Eideticker, the question is &amp;#8212; how can we break the frame data we&amp;#8217;re gathering down into a form that&amp;#8217;s amenable to this kind of analysis? The approach I took (on the recommendation of &lt;a href="http://brainacle.com/calculating-image-entropy-with-python-how-and-why.html"&gt;this article&lt;/a&gt;) was to create a histogram with 256 bins (representing the number of distinct possibilities in a black &amp;#38; white capture) out of all the pixels in the frame, then run the formula over that. The exact function I wound up using looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def _get_frame_entropy((i, capture, sobelized)):
    frame = capture.get_frame(i, True).astype('float')
    if sobelized:
        frame = ndimage.median_filter(frame, 3)

        dx = ndimage.sobel(frame, 0)  # horizontal derivative
        dy = ndimage.sobel(frame, 1)  # vertical derivative
        frame = numpy.hypot(dx, dy)  # magnitude
        frame *= 255.0 / numpy.max(frame)  # normalize (Q&amp;amp;D)

    histogram = numpy.histogram(frame, bins=256)[0]
    histogram_length = sum(histogram)
    samples_probability = [float(h) / histogram_length for h in histogram]
    entropy = -sum([p * math.log(p, 2) for p in samples_probability if p != 0])

    return entropy

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href="https://github.com/mozilla/eideticker/blob/master/src/videocapture/videocapture/entropy.py#L10"&gt;[Context]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The &amp;#8220;sobelized&amp;#8221; bit allows us to optionally convolve the frame with a sobel filter before running the entropy calculation, which removes most of the data in the capture except for the edges. This is especially useful for FirefoxOS, where the signal has quite a bit of random noise from ambient lighting that artificially inflate the entropy values even in places where there is little actual &amp;#8220;information&amp;#8221;.&lt;/p&gt;

&lt;p&gt;This type of transformation often reveals very interesting information about what&amp;#8217;s going on in an eideticker test. For example, take this video of the user panning down in the contacts app:&lt;/p&gt;

&lt;div style="width: 640px; " class="wp-video"&gt;
 &lt;video class="wp-video-shortcode" id="video-1012-2" width="640" height="917" preload="metadata" controls="controls"&gt;
  &lt;source type="video/webm" src="/files/2014/03/contacts-scrolling-movie.webm?_=2" /&gt;&lt;a href="/files/2014/03/contacts-scrolling-movie.webm"&gt;/files/2014/03/contacts-scrolling-movie.webm&lt;/a&gt;&lt;/video&gt;&lt;/div&gt;

&lt;p&gt;If you graph the entropies of the frame of the capture using the formula above you, you get a graph like this:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/03/contacts-scrolling-entropy-graph.png"&gt;&lt;img src="/files/2014/03/contacts-scrolling-entropy-graph.png" alt="contacts scrolling entropy graph" width="933" height="482" class="alignnone size-full wp-image-1022" srcset="/files/2014/03/contacts-scrolling-entropy-graph-300x154.png 300w, /files/2014/03/contacts-scrolling-entropy-graph.png 933w" sizes="(max-width: 933px) 100vw, 933px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="http://eideticker.wrla.ch/b2g/detail.html?id=3f7b7c88a9ed11e380c5f0def1767b24#/framesobelentropies"&gt;[Link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The Y axis represents entropy, as calculated by the code above. There is no inherently &amp;#8220;right&amp;#8221; value for this &amp;#8212; it all depends on the application you&amp;#8217;re testing and what you expect to see displayed on the screen. In general though, higher values are better as it indicates more frames of the capture are &amp;#8220;complete&amp;#8221;.&lt;/p&gt;

&lt;p&gt;The region at the beginning where it is at about 5.0 represents the contacts app with a set of contacts fully displayed (at startup). The &amp;#8220;flat&amp;#8221; regions where the entropy is at roughly 4.25? Those are the areas where the app is &amp;#8220;checkerboarding&amp;#8221; (blanking out waiting for graphics or layout engine to draw contact information). Click through to the original and swipe over the graph to see what I mean.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s easy to see what a hypothetical ideal end state would be for this capture: a graph with a smooth entropy of about 5.0 (similar to the start state, where all contacts are fully drawn in). We can track our progress towards this goal (or our deviation from it), by watching the eideticker b2g dashboard and seeing if the summation of the entropy values for frames over the entire test increases or decreases over time. If we see it generally increase, that probably means we&amp;#8217;re seeing less checkerboarding in the capture. If we see it decrease, that might mean we&amp;#8217;re now seeing checkerboarding where we weren&amp;#8217;t before.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s too early to say for sure, but over the past few days the trend has been positive:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/03/entropy-levels-climbing.png"&gt;&lt;img src="/files/2014/03/entropy-levels-climbing.png" alt="entropy-levels-climbing" width="822" height="529" class="alignnone size-full wp-image-1025" srcset="/files/2014/03/entropy-levels-climbing-300x193.png 300w, /files/2014/03/entropy-levels-climbing.png 822w" sizes="(max-width: 822px) 100vw, 822px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="http://eideticker.wrla.ch/b2g/#/inari/b2g-contacts-scrolling/overallentropy"&gt;[Link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(note that there were some problems in the way the tests were being run before, so results before the 12th should not be considered valid)&lt;/p&gt;

&lt;p&gt;So one concept, at least two relevant metrics we can measure with it (startup time and checkerboarding). Are there any more? Almost certainly, let&amp;#8217;s find them!&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Eideticker for FirefoxOS: Becoming more useful</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2014/03/eideticker-for-firefoxos-becoming-more-useful?utm_source=FirefoxOS&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2014-03-eideticker-for-firefoxos-becoming-more-useful</id>
  <published>2014-03-09T05:00:00Z</published>
  <updated>2014-03-09T05:00:00Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Time for a long overdue eideticker-for-firefoxos update. &lt;a href="http://wrla.ch/blog/2013/10/automatically-measuring-startup-load-time-with-eideticker/"&gt;Last time we were here&lt;/a&gt; (almost 5 months ago! man time flies), I was discussing methodologies for measuring startup performance. Since then, &lt;a href="http://blargon7.com/"&gt;Dave Hunt&lt;/a&gt; and myself have been doing lots of work to make Eideticker more robust and useful. Notably, we now have a setup in London running a suite of Eideticker tests on the latest version of FirefoxOS on the Inari on a daily basis, reporting to &lt;a href="http://eideticker.mozilla.org/b2g"&gt;http://eideticker.mozilla.org/b2g&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/03/b2g-contacts-startup-dashboard.png"&gt;&lt;img src="/files/2014/03/b2g-contacts-startup-dashboard.png" alt="b2g-contacts-startup-dashboard" width="840" height="601" class="alignnone size-full wp-image-1005" srcset="/files/2014/03/b2g-contacts-startup-dashboard-300x214.png 300w, /files/2014/03/b2g-contacts-startup-dashboard.png 840w" sizes="(max-width: 840px) 100vw, 840px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There were more than a few false starts with and some of the earlier data is not to be entirely trusted&amp;#8230; but it now seems to be chugging along nicely, hopefully providing startup numbers that provide a useful counterpoint to the &lt;a href="https://datazilla.mozilla.org/b2g"&gt;datazilla startup numbers&lt;/a&gt; we&amp;#8217;ve already been collecting for some time. There still seem to be some minor problems, but in general I am becoming more and more confident in it as time goes on.&lt;/p&gt;

&lt;p&gt;One feature that I am particularly proud of is the detail view, which enables you to see frame-by-frame what&amp;#8217;s going on. Click on any datapoint on the graph, then open up the view that gives an account of what eideticker is measuring. Hover over the graph and you can see what the video looks like at any point in the capture. This not only lets you know that something regressed, but how. For example, in the messages app, you can scan through this view to see exactly when the first message shows up, and what exact state the application is in when Eideticker says it&amp;#8217;s &amp;#8220;done loading&amp;#8221;.&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/03/capture-detail-view.png"&gt;&lt;img src="/files/2014/03/capture-detail-view.png" alt="Capture Detail View" width="964" height="843" class="alignnone size-full wp-image-1008" srcset="/files/2014/03/capture-detail-view-300x262.png 300w, /files/2014/03/capture-detail-view.png 964w" sizes="(max-width: 964px) 100vw, 964px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="http://eideticker.wrla.ch/b2g/framediff.html?id=3819a484a6d611e3ab89f0def1767b24"&gt;[link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(apologies for the low quality of the video &amp;#8212; should be fixed with &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=980479"&gt;this bug&lt;/a&gt; next week)&lt;/p&gt;

&lt;p&gt;As it turns out, this view has also proven to be particularly useful when working with the new entropy measurements in Eideticker which I&amp;#8217;ve been using to measure checkerboarding (redraw delay) on FirefoxOS. More on that next week.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Automatically measuring startup / load time with Eideticker</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2013/10/automatically-measuring-startup-load-time-with-eideticker?utm_source=FirefoxOS&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2013-10-automatically-measuring-startup-load-time-with-eideticker</id>
  <published>2013-10-17T04:00:00Z</published>
  <updated>2013-10-17T04:00:00Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;So we&amp;#8217;ve been using Eideticker to automatically measure startup/pageload times for about a year now on Android, and more recently on FirefoxOS as well (albeit not automatically). This gives us nice and pretty graphs like this:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2013/10/flot-startup-times-gn.png"&gt;&lt;img src="/files/2013/10/flot-startup-times-gn.png" alt="flot-startup-times-gn" width="620" height="568" class="alignnone size-full wp-image-986" srcset="/files/2013/10/flot-startup-times-gn-300x274.png 300w, /files/2013/10/flot-startup-times-gn.png 620w" sizes="(max-width: 620px) 100vw, 620px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ok, so we&amp;#8217;re generating numbers and graphing them. That&amp;#8217;s great. But what&amp;#8217;s really going on behind the scenes? I&amp;#8217;m glad you asked. The story is a bit different depending on which platform you&amp;#8217;re talking about.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Android&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;On Android we connect Eideticker to the device&amp;#8217;s HDMI out, so we count on a nearly pixel-perfect signal. In practice, it isn&amp;#8217;t quite, but it is within a few RGB values that we can easily filter for. This lets us come up with a pretty good mechanism for determining when a page load or app startup is finished: just compare frames, and say we&amp;#8217;ve &amp;#8220;stopped&amp;#8221; when the pixel differences between frames are negligible (previously defined at 2048 pixels, now 4096 &amp;#8212; see below). Eideticker&amp;#8217;s new frame difference view lets us see how this works. Look at this graph of application startup:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2013/10/frame-difference-android-startup.png"&gt;&lt;img src="/files/2013/10/frame-difference-android-startup.png" alt="frame-difference-android-startup" width="803" height="514" class="alignnone size-full wp-image-973" srcset="/files/2013/10/frame-difference-android-startup-300x192.png 300w, /files/2013/10/frame-difference-android-startup.png 803w" sizes="(max-width: 803px) 100vw, 803px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="http://eideticker.wrla.ch/#/samsung-gn/startup-abouthome-dirty/timetostableframe"&gt;[Link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;What&amp;#8217;s going on here? Well, we see some huge jumps in the beginning. This represents the animated transitions that Android makes as we transition from the SUTAgent application (don&amp;#8217;t ask) to the beginnings of the FirefoxOS browser chrome. You&amp;#8217;ll notice though that there&amp;#8217;s some more changes that come in around the 3 second mark. This is when the site bookmarks are fully loaded. If you load the original page (link above) and swipe your mouse over the graph, you can see what&amp;#8217;s going on for yourself.&lt;/p&gt;

&lt;p&gt;This approach is not completely without problems. It turns out that there is sometimes some minor churn in the display even when the app is for all intents and purposes started. For example, &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=922770"&gt;sometimes the scrollbar fading out of view can result in a significantish pixel value change&lt;/a&gt;, so I recently upped the threshold of pixels that are different from 2048 to 4096. We also recently encountered a &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=926997"&gt;silly problem&lt;/a&gt; with a random automation app displaying &amp;#8220;toasts&amp;#8221; which caused results to artificially spike. More tweaking may still be required. However, on the whole I&amp;#8217;m pretty happy with this solution. It gives useful, undeniably objective results whose meaning is easy to understand.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FirefoxOS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So as mentioned previously, we use a camera on FirefoxOS to record output instead of HDMI output. Pretty unsurprisingly, this is much noisier. See this movie of the contacts app starting and note all the random lighting changes, for example:&lt;/p&gt;

&lt;div style="width: 409px; " class="wp-video"&gt;&lt;!--[if lt IE 9]&gt;&lt;![endif]--&gt;
 &lt;video class="wp-video-shortcode" id="video-972-1" width="409" height="580" preload="metadata" controls="controls"&gt;
  &lt;source type="video/webm" src="/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm?_=1" /&gt; &lt;a href="/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm"&gt;/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm&lt;/a&gt;&lt;/video&gt;&lt;/div&gt;

&lt;p&gt;My experience has been that pixel differences can be so great between visually identical frames on an eideticker capture on these devices that it&amp;#8217;s pretty much impossible to settle on when startup is done using the frame difference method. It&amp;#8217;s of course possible to detect very large scale changes, but the small scale ones (like the contacts actually appearing in the example above) are very hard to distinguish from random differences in the amount of light absorbed by the camera sensor. Tricks like using median filtering (a.k.a. &amp;#8220;blurring&amp;#8221;) help a bit, but not much. Take a look at this graph, for example:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2013/10/plotly-contacts-load-pixeldiff.png"&gt;&lt;img src="/files/2013/10/plotly-contacts-load-pixeldiff.png" alt="plotly-contacts-load-pixeldiff" width="531" height="679" class="alignnone size-full wp-image-980" srcset="/files/2013/10/plotly-contacts-load-pixeldiff-234x300.png 234w, /files/2013/10/plotly-contacts-load-pixeldiff.png 531w" sizes="(max-width: 531px) 100vw, 531px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="https://plot.ly/~WilliamLachance/3"&gt;[Link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You&amp;#8217;ll note that the pixel differences during &amp;#8220;static&amp;#8221; parts of the capture are highly variable. This is because the pixel difference depends heavily on how &amp;#8220;bright&amp;#8221; each frame is: parts of the capture which are black (e.g. a contacts icon with a black background) have a much lower difference between them than parts that are bright (e.g. the contacts screen fully loaded).&lt;/p&gt;

&lt;p&gt;After a day or so of experimenting and research, I settled on an approach which seems to work pretty reliably. Instead of comparing the frames directly, I measure the &lt;a href="http://en.wikipedia.org/wiki/Entropy"&gt;entropy&lt;/a&gt; of the &lt;a href="http://en.wikipedia.org/wiki/Image_histogram"&gt;histogram&lt;/a&gt; of colours used in each frame (essentially just an indication of brightness in this case, see &lt;a href="http://brainacle.com/calculating-image-entropy-with-python-how-and-why.html"&gt;this article&lt;/a&gt; for more on calculating it), then compare that of each frame with the average of the same measure over 5 previous frames (to account for the fact that two frames may be arbitrarily different, but that is unlikely that a sequence of frames will be). This seems to work much better than frame difference in this environment: although there are plenty of minute differences in light absorption in a capture from this camera, the overall color composition stays mostly the same. See this graph:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2013/10/plotly-contacts-load-entropy.png"&gt;&lt;img src="/files/2013/10/plotly-contacts-load-entropy.png" alt="plotly-contacts-load-entropy" width="546" height="674" class="alignnone size-full wp-image-979" srcset="/files/2013/10/plotly-contacts-load-entropy-243x300.png 243w, /files/2013/10/plotly-contacts-load-entropy.png 546w" sizes="(max-width: 546px) 100vw, 546px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="https://plot.ly/~WilliamLachance/5"&gt;[Link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you look closely, you can see some minor variance in the entropy differences depending on the state of the screen, but it&amp;#8217;s not nearly as pronounced as before. In practice, I&amp;#8217;ve been able to get extremely consistent numbers with a reasonable &amp;#8220;threshold&amp;#8221; of &amp;#8220;0.05&amp;#8221;.&lt;/p&gt;

&lt;p&gt;In Eideticker I&amp;#8217;ve tried to steer away from using really complicated math or algorithms to measure things, unless all the alternatives fail. In that sense, I really liked the simplicity of &amp;#8220;pixel differences&amp;#8221; and am not thrilled about having to resort to this: hopefully the concepts in this case (histograms and entropy) are simple enough that most people will be able to understand my methodology, if they care to. Likely I will need to come up with something else for measuring responsiveness and animation smoothness (frames per second), as likely we can&amp;#8217;t count on light composition changing the same way for those cases. My initial thought was to use &lt;a href="http://en.wikipedia.org/wiki/Edge_detection"&gt;edge detection&lt;/a&gt; (which, while somewhat complex to calculate, is at least easy to understand conceptually) but am open to other ideas.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Simple command-line ntp client for Android and FirefoxOS</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2013/07/simple-command-line-ntp-client-for-android-and-firefoxos?utm_source=FirefoxOS&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2013-07-simple-command-line-ntp-client-for-android-and-firefoxos</id>
  <published>2013-07-08T04:00:00Z</published>
  <updated>2013-07-08T04:00:00Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;Today I did a quick port of Larry Doolittle&amp;#8217;s &lt;a href="http://doolittle.icarus.com/ntpclient/"&gt;ntpclient program&lt;/a&gt; to Android and FirefoxOS. Basically this lets you easily synchronize your device&amp;#8217;s time to that of a central server. Yes, there&amp;#8217;s lots and lots of Android &amp;#8220;applications&amp;#8221; which let you do this, but I wanted to be able to do this from the command line because that&amp;#8217;s how I roll. If you&amp;#8217;re interested, source and instructions are here:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://github.com/wlach/ntpclient-android"&gt;https://github.com/wlach/ntpclient-android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For those curious, no, I didn&amp;#8217;t just do this for fun. For next quarter, we want to write some Eideticker-based responsiveness tests for FirefoxOS and Android. For example, how long does it take from the time you tap on an icon in the homescreen on FirefoxOS to when the application is fully loaded? Or on Android, how long does it take to see a full list of sites in the awesomebar from the time you tap on the URL field and enter your search term?&lt;/p&gt;

&lt;p&gt;Because an Eideticker test run involves two different machines (a host machine which controls the device and captures video of it in action, as well as the device itself), we need to use timestamps to really understand when and how events are being sent to the device. To do that reliably, we really need some easy way of synchronizing time between two machines (or at least accounting for the difference in their clocks, which amounts to about the same thing). NTP struck me as being the easiest, most standard way of doing this.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Proof of concept Eideticker dashboard for FirefoxOS</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2013/05/proof-of-concept-eideticker-dashboard-for-firefoxos?utm_source=FirefoxOS&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2013-05-proof-of-concept-eideticker-dashboard-for-firefoxos</id>
  <published>2013-05-06T04:00:00Z</published>
  <updated>2013-05-06T04:00:00Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I just put up a proof of concept Eideticker dashboard for FirefoxOS &lt;a href="http://eideticker.wrla.ch/b2g"&gt;here&lt;/a&gt;. Right now it has two days worth of data, manually sampled from an Unagi device running b2g18. Right now there are two tests: one the measures the &amp;#8220;speed&amp;#8221; of the contacts application scrolling, another that measures the amount of time it takes for the contacts application to be fully loaded.&lt;/p&gt;

&lt;p&gt;For those not already familiar with it, Eideticker is a benchmarking suite which captures live video data coming from a device and analyzes it to determine performance. This lets us get data which is more representative of actual user experience (as opposed to an oft artificial benchmark). For example, Eideticker measures contacts startup as taking anywhere between 3.5 seconds and 4.5 seconds, versus than the 0.5 to 1 seconds that the &lt;a href="https://datazilla.mozilla.org/b2g/?branch=master&amp;amp;#038;range=7&amp;amp;#038;test=cold_load_time&amp;amp;#038;app_list=contacts&amp;amp;#038;app=contacts&amp;amp;#038;gaia_rev=114bf216de0a19f7&amp;amp;#038;gecko_rev=9c0de2afd22a8476"&gt;existing datazilla benchmarks&lt;/a&gt; show. What accounts for the difference? If you step through an eideticker-captured video, you can see that even though &lt;em&gt;something&lt;/em&gt; appears very quickly, not all the contacts are displayed until the 3.5 second mark. There is a gap between an app being reported as &amp;#8220;loaded&amp;#8221; and it being fully available for use, which we had not been measuring until now.&lt;/p&gt;

&lt;p&gt;At this point, I am most interested in hearing from FirefoxOS developers on new tests that would be interesting and useful to track performance of the system on an ongoing basis. I&amp;#8217;d obviously prefer to focus on things which have been difficult to measure accurately through other means. My setup is rather fiddly right now, but hopefully soon we can get some useful numbers going on an ongoing basis, as we &lt;a href="http://eideticker.wrla.ch"&gt;do already&lt;/a&gt; for Firefox for Android.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Actual useful FirefoxOS Eideticker results at last</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2013/04/actual-useful-firefoxos-eideticker-results-at-last?utm_source=FirefoxOS&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2013-04-actual-useful-firefoxos-eideticker-results-at-last</id>
  <published>2013-04-22T04:00:00Z</published>
  <updated>2013-04-22T04:00:00Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;Another update on getting &lt;a href="http://wrla.ch/blog/2013/02/eideticker-for-firefoxos/"&gt;Eideticker working with FirefoxOS&lt;/a&gt;. Once again this is sort of high-level, looking forward to writing something more in-depth soon now that we have the basics working.&lt;/p&gt;

&lt;p&gt;I finally got the last kinks out of the rig I was using to capture live video from FirefoxOS phones using the Point Grey devices last week. In order to make things reasonable I had to write some custom code to isolate the actual device screen from the rest of capture and a few other things. The setup looks interesting (reminds me a bit of something out of the War of the Worlds):&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2013/04/eideticker-pointgrey-mounted.jpg"&gt;&lt;img src="/files/2013/04/eideticker-pointgrey-mounted.jpg" alt="eideticker-pointgrey-mounted" width="512" height="683" class="alignnone size-full wp-image-894" srcset="/files/2013/04/eideticker-pointgrey-mounted-224x300.jpg 224w, /files/2013/04/eideticker-pointgrey-mounted.jpg 512w" sizes="(max-width: 512px) 100vw, 512px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s some example video of a test I wrote up to measure the performance of contacts scrolling performance (measured at a very respectable 44 frames per second, in case you wondering):&lt;/p&gt;

&lt;video src="/files/eideticker/contacts-scrolling-pointgrey.webm" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;Surprisingly enough, I didn&amp;#8217;t wind up having to write up any code to compensate for a noisy image. Of course there&amp;#8217;s a certain amount of variance in every frame depending on how much light is hitting the camera sensor at any particular moment, but apparently not enough to interfere with getting useful results in the tests I&amp;#8217;ve been running.&lt;/p&gt;

&lt;p&gt;Likely next step: Create some kind of chassis for mounting both the camera and device on a permanent basis (instead of an adhoc one on my desk) so we can start running these sorts of tests on a daily basis, much like we currently do with Android on the &lt;a href="http://eideticker.wrla.ch"&gt;Eideticker Dashboard&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As an aside, I&amp;#8217;ve been really impressed with both the &lt;a href="https://wiki.mozilla.org/Auto-tools/Projects/Marionette"&gt;Marionette&lt;/a&gt; framework and the gaiatests python module that was written up for FirefoxOS. Writing the above test took just 5 minutes &amp;#8212; and &lt;a href="https://github.com/mozilla/eideticker/blob/master/src/tests/b2g/appscrolling/scroll.py"&gt;the code&lt;/a&gt; is quite straightforward. Quite the pleasant change from my various efforts in Android automation.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Documentation for mozdevice</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2013/03/documentation-for-mozdevice?utm_source=FirefoxOS&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2013-03-documentation-for-mozdevice</id>
  <published>2013-03-11T04:00:00Z</published>
  <updated>2013-03-11T04:00:00Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;Just wanted to give a quick heads up that as part of the ateam&amp;#8217;s ongoing effort to improve the documentation of our automated testing infrastructure, we now have &lt;a href="https://mozbase.readthedocs.org/en/latest/mozdevice.html"&gt;online documentation&lt;/a&gt; for mozdevice, the python library we use for interacting with Android- and FirefoxOS-based devices in automated testing.&lt;/p&gt;

&lt;p&gt;Mozdevice is used in pretty much every one of our testing frameworks that has mobile support, including mochitest, reftest, &lt;a href="https://wiki.mozilla.org/Buildbot/Talos"&gt;talos&lt;/a&gt;, &lt;a href="https://github.com/mozilla/autophone"&gt;autophone&lt;/a&gt;, and &lt;a href="https://wiki.mozilla.org/Project_Eideticker"&gt;eideticker&lt;/a&gt;. Additionally, mozdevice is used by release engineering to clean up, monitor, and otherwise manage 
 &lt;strike&gt;our hundred-odd&lt;/strike&gt; the 1200*** tegra and panda development boards that we use in &lt;a href="http://tbpl.mozilla.org"&gt;tbpl&lt;/a&gt;. See &lt;a href="https://hg.mozilla.org/build/tools/file/tip/sut_tools"&gt;sut_tools&lt;/a&gt; (old, buildbot-based, what we currently use) and &lt;a href="https://github.com/mozilla/mozpool"&gt;mozpool&lt;/a&gt; (the new and shiny future).&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Thanks to Dustin Mitchell for the correction.&lt;/li&gt;&lt;/ul&gt;</content></entry>
 <entry>
  <title type="text">Finding a camera for Eideticker</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2013/02/finding-a-camera-for-eideticker?utm_source=FirefoxOS&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2013-02-finding-a-camera-for-eideticker</id>
  <published>2013-02-19T05:00:00Z</published>
  <updated>2013-02-19T05:00:00Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Ok, so as I mentioned &lt;a href="http://wrla.ch/blog/2013/02/eideticker-for-firefoxos/"&gt;last time&lt;/a&gt; I&amp;#8217;ve been looking into making Eideticker work for devices without native HDMI output by capturing their output with some kind of camera. So far I&amp;#8217;ve tried four different DSLRs for this task, which have all been inadequate for different reasons. I was originally just going to write an email about this to a few concerned parties, but then figured I may as well structure it into a blog post. Maybe someone will find it useful (or better yet, have some ideas.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Elmo MO&amp;ndash;1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is the device I mentioned last time. Easy to set up, plays nicely with the Decklink capture card we&amp;#8217;re using for Eideticker. It seemed almost perfect, except for that:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;The image quality is &lt;em&gt;really&lt;/em&gt; bad (beaten even by $200 standard digital camera). Tons of noise, picture quality really bad. Not *necessarily* a deal breaker, but it still sucks.&lt;/li&gt;
 &lt;li&gt;More importantly, there seems to be no way of turning off the auto white balance adjustment. This makes automated image analysis impossible if the picture changes, as is highlighted in this video:   
  &lt;video width="400px" src="/files/eideticker/elmo-white-balance-problem.webm" controls="controls"&gt;&lt;/video&gt;&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Canon Rebel T4i&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is the first camera that was recommended to me at the camera shop I&amp;#8217;ve been going to. It does have an HDMI output signal, but it&amp;#8217;s not &amp;#8220;clean&amp;#8221;. This &lt;a href="http://www.hireacamera.com/blog/index.asp?post=canon-eos-650d--hdmi-explained"&gt;blog post&lt;/a&gt; explains the details better than I could. Next.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nikon D600&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Supposedly does native clean 720p output, but unfortunately the &lt;a href="http://vimeo.com/49952287"&gt;output is in a &amp;#8220;box&amp;#8221;&lt;/a&gt; so isn&amp;#8217;t recognized by the Decklink cards that we&amp;#8217;re using (which insist on a full 1280&amp;#215;720 HDMI signal to work). Next.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nikon D800&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It is possible to configure this one to not put a box around the output, so the Decklink card does recognize it. Except that the camera shuts off the HDMI signal whenever the input parameters change on the card or the signal input is turned on, which essentially makes it useless for Eideticker (this happens every time we start the Eideticker harness). Quite a shame, as the HDMI signal is quite nice otherwise.&lt;/p&gt;

&lt;p&gt;&amp;#8212;&lt;/p&gt;

&lt;p&gt;To be clear, with the exception of the Elmo all the devices above seem like fine cameras, and should more than do for manual captures of B2G or Android phones (which is something we probably want to do anyway). But for Eideticker, we need something that works in automation, and none of the above fit the bill. I guess I could explore using a &amp;#8220;real&amp;#8221; video camera as opposed to a DSLR acting like one, though I suspect I might run into some of the same sorts of issues depending on how the HDMI output of those devices behaves.&lt;/p&gt;

&lt;p&gt;Part of me wonders whether a custom solution wouldn&amp;#8217;t work better. How complicated could it be to construct your own digital camera anyway? ðŸ˜‰ Hook up a fancy camera sensor to a &lt;a href="http://pandaboard.org"&gt;pandaboard&lt;/a&gt;, get it to output through the HDMI port, and then we&amp;#8217;re set? Or better yet, maybe just get a fancy webcam like the &lt;a href="http://en.wikipedia.org/wiki/PlayStation_Eye"&gt;Playstation Eye&lt;/a&gt; and hook it up directly to a computer? That would eliminate the need for our expensive video capture card setup altogether.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Eideticker for FirefoxOS</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2013/02/eideticker-for-firefoxos?utm_source=FirefoxOS&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2013-02-eideticker-for-firefoxos</id>
  <published>2013-02-01T05:00:00Z</published>
  <updated>2013-02-01T05:00:00Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s a long overdue update on where we&amp;#8217;re at with Eideticker for FirefoxOS. While we&amp;#8217;ve had a good amount of success getting &lt;a href="http://eideticker.wrla.ch"&gt;useful, actionable data&lt;/a&gt; out of Eideticker for Android, so far we haven&amp;#8217;t been able to replicate that success for FirefoxOS. This is not for lack of trying: first, &lt;a href="http://nakubu.com/"&gt;Malini Das&lt;/a&gt; and then me have been working at it since summer 2012.&lt;/p&gt;

&lt;p&gt;When it comes right down to it, instrumenting Eideticker for B2G is just a whole lot more complex. On Android, we could take the operating system (including support for all the things we needed, like HDMI capture) as a given. The only tricky part was instrumenting the capture so the right things happened at the right moment. With FirefoxOS, we need to run these tests on entire builds of a whole operating system which was constantly changing. Not nearly as simple. That said, I&amp;#8217;m starting to see light at the end of the tunnel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Platforms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We initially selected the &lt;a href="http://pandaboard.org"&gt;pandaboard&lt;/a&gt; as the main device to use for eideticker testing, for two reasons. First, it&amp;#8217;s the same hardware platform we&amp;#8217;re targeting for other b2g testing in tbpl (mochitest, reftest, etc.), and is the platform we&amp;#8217;re using for running Gaia UI tests. Second, unlike every other device that we&amp;#8217;re prototyping FirefoxOS on (to my knowledge), it has HDMI-out capability, so we can directly interface it with the Eideticker video capture setup.&lt;/p&gt;

&lt;p&gt;However, the panda also has some serious shortcomings. First, it&amp;#8217;s obviously not a platform we&amp;#8217;re shipping, so the performance we&amp;#8217;re seeing from it is subject to different factors that we might not see with a phone actually shipped to users. For the same reason, we&amp;#8217;ve had many problems getting B2G running reliably on it, as it&amp;#8217;s not something most developers have been hacking on a day to day basis. Thanks to the heroic efforts of Thomas Zimmerman, we&amp;#8217;ve mostly got things working ok now, but it was a fairly long road to get here (several months last fall).&lt;/p&gt;

&lt;p&gt;More recently, we became aware of something called an &lt;a href="http://www.elmousa.com/"&gt;Elmo&lt;/a&gt; which might let us combine
 &lt;br /&gt;the best of both worlds. An elmo is really just a tiny mounted video camera with a bunch of outputs, and is I understand most commonly used to project documents in a classroom/presentation setting. However, it seems to do a great job of capturing mobile phones in action as well.&lt;/p&gt;

&lt;video width="400px" src="/files/eideticker/startup-test-elmo.webm" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;The nice thing about using an external camera for the video capture part of eideticker is that we are no longer limited to devices with HDMI out &amp;#8212; we can run the standard set of automated tests on ANYTHING. We&amp;#8217;ve already used this to some success in getting some videos of FirefoxOS startup times versus Android on the Unagi (a development phone that we&amp;#8217;re using internally) for manual analysis. Automating this process may be trickier because of the fact that the video capture is no longer &amp;#8220;perfect&amp;#8221;, but we may be able to work around that (more discussion about this later).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FirefoxOS web page tests&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These are the same tests we run on Android. They should give us an idea of roughly where our performance when browsing / panning web sites like CNN. So far, I&amp;#8217;ve only run these tests on the Pandaboard and they are INCREDIBLY slow (like 1&amp;ndash;3 frames per second when scrolling). So much so that I have to think there is something broken about our hardware acceleration on this platform.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FirefoxOS application tests&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These are some new tests written in a framework that allows you to script arbitrary interactions in FirefoxOS, like launching applications or opening the task switcher.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m pretty happy with this. It seems to work well. The only problems I&amp;#8217;m seeing with this is with the platform we&amp;#8217;re running these tests on. With the pandaboard, applications look weird (since the screen resolution doesn&amp;#8217;t remotely resemble the 320&amp;#215;480 resolution on our current devices) and performance is abysmal. Take, for example, this capture of application switching performance, which operates only at roughly 3&amp;ndash;4 fps:&lt;/p&gt;

&lt;video width="400px" src="/files/eideticker/b2g-appswitching-video.webm" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;&lt;strong&gt;So what now?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m not 100% sure yet (partly it will depend on what others say as well as my own investigation), but I have a feeling that capturing video of real devices running FirefoxOS using the Elmo is the way forward. First, the hardware and driver situation will be much more representative of what we&amp;#8217;ll actually be shipping to users. Second, we can flash new builds of FirefoxOS onto them automatically, unlike the pandaboards where you currently either need to manually flash and reset (a time consuming and error prone process) or set up an instance of &lt;a href="https://github.com/djmitche/mozpool"&gt;mozpool&lt;/a&gt; (which I understand is quite complicated).&lt;/p&gt;

&lt;p&gt;The main use case I see with eideticker-on-panda would be where we wanted to run a suite of tests on checkin (in tbpl-like fashion) and we&amp;#8217;d need to scale to many devices. While cool, this sounds like an expensive project (both in terms of time and hardware) and I think we&amp;#8217;d do better with getting something slightly smaller-scale running first.&lt;/p&gt;

&lt;p&gt;So, the real question is whether or not the capture produced by the Elmo is amenable to the same analysis that we do on the raw HDMI output. At the very least, some of eideticker&amp;#8217;s image analysis code will have to be adapted to handle a much &amp;#8220;noisier&amp;#8221; capture. As opposed to capturing the raw HDMI signal, we now have to deal with the real world and its irritating fluctuations in ambient light levels and all that the rest. I have no doubt it is *possible* to compensate for this (after all this is what the human eye/brain does all the time), but the question is how much work it will be. Can&amp;#8217;t speak for anyone else at Mozilla, but I&amp;#8217;m not sure if I really have the time to start a Ph.D-level research project in computational vision. ðŸ˜‰ I&amp;#8217;m optimistic that won&amp;#8217;t be necessary, but we&amp;#8217;ll just have to wait and see.&lt;/p&gt;</content></entry></feed>