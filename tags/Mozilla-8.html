<!DOCTYPE html>
<html lang="en">
  <head>

    <meta charset="utf-8" />
    <title>Posts tagged 'Mozilla' (page 8)</title>
    <meta name="description" content="Posts tagged 'Mozilla' (page 8)" />
    <meta name="author" content="William Lachance" />
    <meta name="keywords" content="Mozilla" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="/favicon.ico" />
    <link rel="canonical" href="https://wrla.ch/tags/Mozilla-8.html" />

    <!-- CSS -->
    <link rel="stylesheet" type="text/css" href="/css/style.css" />
    <link
      rel="stylesheet"
      type="text/css"
      href="/css/pygments.css"
    />
    <link
      rel="stylesheet"
      type="text/css"
      href="/css/scribble.css"
    />
    <!-- Feeds -->
    <link
      rel="alternate"
      type="application/atom+xml"
      href="/feeds/Mozilla.atom.xml"
      title="Atom Feed"
    />
    <link
      rel="alternate"
      type="application/rss+xml"
      href="/feeds/Mozilla.rss.xml"
      title="RSS Feed"
    />
    <!-- JS -->
    <script type="text/javascript">
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-xxxxx', 'auto');
      ga('send', 'pageview');
    </script>
  </head>
  <body>
    <nav
      class="flex items-center justify-between flex-wrap bg-gray-800 py-1 px-8"
    >
      <div class="flex items-center flex-shrink-0 text-gray-400 mr-6">
        <div class="p-1">
          <a href="/index.html"
            ><img
              src="/img/wlach_icon.png"
              width="32"
              height="32"
              class="p rounded"
          /></a>
        </div>
        <div class="p-1">
          <a
            href="/index.html"
            class="text-gray-200 font-semibold text-xl tracking-tight hover:text-white"
            >wlach log</a
          >
        </div>
      </div>
      <div class="flex-grow lg:flex lg:items-center">
        <div class="text-sm lg:flex-grow">
          <a
            href="/About.html"
            class="mt-4 lg:inline-block lg:mt-0 hover:text-white mr-4 text-gray-600"
          >
            About</a>
          <a
            class="mt-4 lg:inline-block lg:mt-0 text-gray-600 hover:text-white mr-4"
            href="/feeds/Mozilla.atom.xml"
            >Atom</a
          >
          <a
            class="mt-4 lg:inline-block lg:mt-0 text-gray-600 hover:text-white mr-4"
            href="/feeds/Mozilla.rss.xml"
            >RSS</a
          >
        </div>
      </div>
    </nav>
    <div id="content" class="container max-w-screen-md px-8 py-4 mx-auto">
       <p class="less-important">Showing posts tagged <em>Mozilla</em></p>  <article>
  <header>
    <h2><a href="/blog/2013/10/automatically-measuring-startup-load-time-with-eideticker/">Automatically measuring startup / load time with Eideticker</a></h2>
    <p class="index-date">Oct 17th, 2013</p>
    <p><span class="tags"><a href="/tags/Data-Visualization.html">Data Visualization</a>  <a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/FirefoxOS.html">FirefoxOS</a>  <a href="/tags/Mozilla.html">Mozilla</a></span></p>
  </header>

<p>So we&rsquo;ve been using Eideticker to automatically measure startup/pageload times for about a year now on Android, and more recently on FirefoxOS as well (albeit not automatically). This gives us nice and pretty graphs like this:</p>

<p><a href="/files/2013/10/flot-startup-times-gn.png"><img src="/files/2013/10/flot-startup-times-gn.png" alt="flot-startup-times-gn" width="620" height="568" class="alignnone size-full wp-image-986" srcset="/files/2013/10/flot-startup-times-gn-300x274.png 300w, /files/2013/10/flot-startup-times-gn.png 620w" sizes="(max-width: 620px) 100vw, 620px" /></a></p>

<p>Ok, so we&rsquo;re generating numbers and graphing them. That&rsquo;s great. But what&rsquo;s really going on behind the scenes? I&rsquo;m glad you asked. The story is a bit different depending on which platform you&rsquo;re talking about.</p>

<p><strong>Android</strong></p>

<p>On Android we connect Eideticker to the device&rsquo;s HDMI out, so we count on a nearly pixel-perfect signal. In practice, it isn&rsquo;t quite, but it is within a few RGB values that we can easily filter for. This lets us come up with a pretty good mechanism for determining when a page load or app startup is finished: just compare frames, and say we&rsquo;ve &ldquo;stopped&rdquo; when the pixel differences between frames are negligible (previously defined at 2048 pixels, now 4096 &#8212; see below). Eideticker&rsquo;s new frame difference view lets us see how this works. Look at this graph of application startup:</p>

<p><a href="/files/2013/10/frame-difference-android-startup.png"><img src="/files/2013/10/frame-difference-android-startup.png" alt="frame-difference-android-startup" width="803" height="514" class="alignnone size-full wp-image-973" srcset="/files/2013/10/frame-difference-android-startup-300x192.png 300w, /files/2013/10/frame-difference-android-startup.png 803w" sizes="(max-width: 803px) 100vw, 803px" /></a>
 <br /><a href="http://eideticker.wrla.ch/#/samsung-gn/startup-abouthome-dirty/timetostableframe">[Link to original]</a></p>

<p>What&rsquo;s going on here? Well, we see some huge jumps in the beginning. This represents the animated transitions that Android makes as we transition from the SUTAgent application (don&rsquo;t ask) to the beginnings of the FirefoxOS browser chrome. You&rsquo;ll notice though that there&rsquo;s some more changes that come in around the 3 second mark. This is when the site bookmarks are fully loaded. If you load the original page (link above) and swipe your mouse over the graph, you can see what&rsquo;s going on for yourself.</p>

<p>This approach is not completely without problems. It turns out that there is sometimes some minor churn in the display even when the app is for all intents and purposes started. For example, <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=922770">sometimes the scrollbar fading out of view can result in a significantish pixel value change</a>, so I recently upped the threshold of pixels that are different from 2048 to 4096. We also recently encountered a <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=926997">silly problem</a> with a random automation app displaying &ldquo;toasts&rdquo; which caused results to artificially spike. More tweaking may still be required. However, on the whole I&rsquo;m pretty happy with this solution. It gives useful, undeniably objective results whose meaning is easy to understand.</p>

<p><strong>FirefoxOS</strong></p>

<p>So as mentioned previously, we use a camera on FirefoxOS to record output instead of HDMI output. Pretty unsurprisingly, this is much noisier. See this movie of the contacts app starting and note all the random lighting changes, for example:</p>

<div style="width: 409px; " class="wp-video"><!--[if lt IE 9]><![endif]-->
 <video class="wp-video-shortcode" id="video-972-1" width="409" height="580" preload="metadata" controls="controls">
  <source type="video/webm" src="/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm?_=1" /> <a href="/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm">/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm</a></video></div>

<p>My experience has been that pixel differences can be so great between visually identical frames on an eideticker capture on these devices that it&rsquo;s pretty much impossible to settle on when startup is done using the frame difference method. It&rsquo;s of course possible to detect very large scale changes, but the small scale ones (like the contacts actually appearing in the example above) are very hard to distinguish from random differences in the amount of light absorbed by the camera sensor. Tricks like using median filtering (a.k.a. &ldquo;blurring&rdquo;) help a bit, but not much. Take a look at this graph, for example:</p>

<p><a href="/files/2013/10/plotly-contacts-load-pixeldiff.png"><img src="/files/2013/10/plotly-contacts-load-pixeldiff.png" alt="plotly-contacts-load-pixeldiff" width="531" height="679" class="alignnone size-full wp-image-980" srcset="/files/2013/10/plotly-contacts-load-pixeldiff-234x300.png 234w, /files/2013/10/plotly-contacts-load-pixeldiff.png 531w" sizes="(max-width: 531px) 100vw, 531px" /></a>
 <br /><a href="https://plot.ly/~WilliamLachance/3">[Link to original]</a></p>

<p>You&rsquo;ll note that the pixel differences during &ldquo;static&rdquo; parts of the capture are highly variable. This is because the pixel difference depends heavily on how &ldquo;bright&rdquo; each frame is: parts of the capture which are black (e.g. a contacts icon with a black background) have a much lower difference between them than parts that are bright (e.g. the contacts screen fully loaded).</p>

<p>After a day or so of experimenting and research, I settled on an approach which seems to work pretty reliably. Instead of comparing the frames directly, I measure the <a href="http://en.wikipedia.org/wiki/Entropy">entropy</a> of the <a href="http://en.wikipedia.org/wiki/Image_histogram">histogram</a> of colours used in each frame (essentially just an indication of brightness in this case, see <a href="http://brainacle.com/calculating-image-entropy-with-python-how-and-why.html">this article</a> for more on calculating it), then compare that of each frame with the average of the same measure over 5 previous frames (to account for the fact that two frames may be arbitrarily different, but that is unlikely that a sequence of frames will be). This seems to work much better than frame difference in this environment: although there are plenty of minute differences in light absorption in a capture from this camera, the overall color composition stays mostly the same. See this graph:</p>

<p><a href="/files/2013/10/plotly-contacts-load-entropy.png"><img src="/files/2013/10/plotly-contacts-load-entropy.png" alt="plotly-contacts-load-entropy" width="546" height="674" class="alignnone size-full wp-image-979" srcset="/files/2013/10/plotly-contacts-load-entropy-243x300.png 243w, /files/2013/10/plotly-contacts-load-entropy.png 546w" sizes="(max-width: 546px) 100vw, 546px" /></a>
 <br /><a href="https://plot.ly/~WilliamLachance/5">[Link to original]</a></p>

<p>If you look closely, you can see some minor variance in the entropy differences depending on the state of the screen, but it&rsquo;s not nearly as pronounced as before. In practice, I&rsquo;ve been able to get extremely consistent numbers with a reasonable &ldquo;threshold&rdquo; of &ldquo;0.05&rdquo;.</p>

<p>In Eideticker I&rsquo;ve tried to steer away from using really complicated math or algorithms to measure things, unless all the alternatives fail. In that sense, I really liked the simplicity of &ldquo;pixel differences&rdquo; and am not thrilled about having to resort to this: hopefully the concepts in this case (histograms and entropy) are simple enough that most people will be able to understand my methodology, if they care to. Likely I will need to come up with something else for measuring responsiveness and animation smoothness (frames per second), as likely we can&rsquo;t count on light composition changing the same way for those cases. My initial thought was to use <a href="http://en.wikipedia.org/wiki/Edge_detection">edge detection</a> (which, while somewhat complex to calculate, is at least easy to understand conceptually) but am open to other ideas.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/10/first-eideticker-responsiveness-tests/">First Eideticker Responsiveness Tests</a></h2>
    <p class="index-date">Oct 7th, 2013</p>
    <p><span class="tags"><a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/Mozilla.html">Mozilla</a>  <a href="/tags/Responsiveness.html">Responsiveness</a></span></p>
  </header>

<p><em>[ For more information on the Eideticker software I&rsquo;m referring to, see <a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/">this entry</a> ]</em></p>

<p>Time for another update on Eideticker. In the last quarter, I&rsquo;ve been working on two main items:</p>

<ol>
 <li>Responsiveness tests (Android / FirefoxOS)</li>
 <li>Eideticker for FirefoxOS</li></ol>

<p>The focus of this post is the responsiveness work. I&rsquo;ll talk about Eideticker for FirefoxOS soon.</p>

<p>So what do I mean by responsiveness? At a high-level, I mean how quickly one sees a response after performing an action on the device. For example, if I perform a swipe gesture to scroll the content down while browsing CNN.com, how long does it take after
 <br />I start the gesture for the content to <em>visibly</em> scroll down? If you break it down, there&rsquo;s a multi-step process that happens behind the scenes after a user action like this:</p>

<p><a href="/files/2013/10/input-events.png"><img src="/files/2013/10/input-events.png" alt="input-events" width="880" height="752" class="alignnone size-full wp-image-957" srcset="/files/2013/10/input-events-300x256.png 300w, /files/2013/10/input-events.png 880w" sizes="(max-width: 880px) 100vw, 880px" /></a></p>

<p>If anywhere in the steps above, there is a significant delay, the user experience is likely to be bad. Usability research
 <br />suggests that any lag that is consistently above 100 milliseconds will lead the user to <a href="http://stackoverflow.com/questions/536300/what-is-the-shortest-perceivable-application-response-delay">perceive things as being laggy</a>. To keep our users happy, we need to do our bit to make sure that we respond quickly at all levels that we control (just the application layer on Android, but pretty much everything on FirefoxOS). Even if we can&rsquo;t complete the work required on our end to completely respond to the user&rsquo;s desire, we should at least display something to acknowledge that things have changed.</p>

<p>But you can&rsquo;t improve what you can&rsquo;t measure. Fortunately, we have the means to do calculate of the time delta between <em>most</em> of the steps above. I learned from <a href="http://taras.glek.net/">Taras Glek</a> this weekend that it should be <a href="http://hackaday.com/2012/05/04/reaching-out-to-a-touch-screen-with-a-microcontroller/">possible to simulate</a> the actual capacitative touch event on a modern touch screen. We can recognize when the hardware event is available to be consumed by userspace by monitoring the `/dev/input` subsystem. And once the event reaches the application (the Android or FirefoxOS application) there&rsquo;s no reason we can&rsquo;t add instrumentation in all sorts of places to track the processing of both the event and the rendering of the response.</p>

<p>My working hypothesis is that it&rsquo;s application-level latency (i.e. the time between the application receiving the event and being able to act on it) that dominates, so that&rsquo;s what I decided to measure. This is purely based on intuition and by no means proven, so we should test this (it would certainly be an interesting exercise!). However, even if it turns out that there are significant problems here, we still care about the other bits of the stack &#8212; there&rsquo;s lots of potentially-latency-introducing churn there and the risk of regression in our own code is probably higher than it is elsewhere since it changes so much.</p>

<p>Last year, I wrote up a tool called <a href="http://wrla.ch/blog/2012/07/the-evolution-of-simulating-events-in-eideticker-from-monkeys-to-orangutns/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-evolution-of-simulating-events-in-eideticker-from-monkeys-to-orangutns">Orangutan</a> that can directly inject input events into an input device on Android or FirefoxOS. It seemed like a fairly straightforward extension of the tool to output timestamps when these events were registered. It was. Then, by <a href="http://wrla.ch/blog/2013/07/simple-command-line-ntp-client-for-android-and-firefoxos/">synchronizing the time</a> between the device and the machine doing the capturing, we can then correlate the input timestamps to events. To help visualize what&rsquo;s going on, I generated this view:</p>

<p><a href="/files/2013/10/taskjs-framediff-view.png"><img src="/files/2013/10/taskjs-framediff-view.png" alt="taskjs-framediff-view" width="583" height="418" class="alignnone size-full wp-image-962" srcset="/files/2013/10/taskjs-framediff-view-300x215.png 300w, /files/2013/10/taskjs-framediff-view.png 583w" sizes="(max-width: 583px) 100vw, 583px" /></a></p>

<p><a href="http://eideticker.wrla.ch/framediff-view.html?title=Frame%20Difference%20Scrolling%20on%20taskjs.org%20%282013-10-06%29&amp;video=videos/video-1381129971.63.webm&amp;framediff=framediffs/framediff-1381129990.79.json&amp;actionlog=actionlogs/action-log-1381129990.79.json">[Link to original]</a></p>

<p>The X axis in that graph represents time. The Y-axis represents the difference between the frame at that time with the previous one in number of pixels. The &ldquo;red&rdquo; represents periods in capture when events are ongoing (we use different colours only to
 <br />distinguish distinct events). <sup><a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/">1</a></sup></p>

<p>For a first pass at measuring responsiveness, I decided to measure the time between the first event being initiated and there being a significant frame difference (i.e. an observable response to the action). You can see some preliminary results on the eideticker dashboard:</p>

<p><a href="/files/2013/10/taskjs-responsiveness.png"><img src="/files/2013/10/taskjs-responsiveness.png" alt="taskjs-responsiveness" width="637" height="540" class="alignnone size-full wp-image-956" srcset="/files/2013/10/taskjs-responsiveness-300x254.png 300w, /files/2013/10/taskjs-responsiveness.png 637w" sizes="(max-width: 637px) 100vw, 637px" /></a></p>

<p><a href="http://eideticker.mozilla.org/#/samsung-gn/taskjs/timetoresponse">[Link to original]</a></p>

<p>The results seem pretty highly variable at first because I was synchronizing time between the device and an external ntp server, rather than the host machine. I believe this is now fixed, hopefully giving us results that will indicate when regressions occur. As time goes by, we may want to craft some special eideticker tests for responsiveness specifically (e.g. a site where there is heavy javascript background processing).</p>

<p><sup><a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/">1</a></sup> <em>Incidentally, these &ldquo;frame difference&rdquo; graphs are also quite useful for understanding where and how application startup has regressed in Fennec &#8212; try opening these two startup views side-by-side (before/after a large regression) and spot the difference: <a href="http://eideticker.wrla.ch/framediff-view.html?title=Frame%20Difference%20Startup%20to%20about:home%20%28dirty%20profile%29%20%282013-08-20%29&amp;video=videos/video-1377070981.95.webm&amp;framediff=framediffs/framediff-1377070991.95.json">[1]</a> and <a href="http://eideticker.wrla.ch/framediff-view.html?title=Frame%20Difference%20Startup%20to%20about:home%20%28dirty%20profile%29%20%282013-08-23%29&amp;video=videos/video-1377330042.28.webm&amp;framediff=framediffs/framediff-1377330051.67.json">[2]</a>)</em></p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/07/simple-command-line-ntp-client-for-android-and-firefoxos/">Simple command-line ntp client for Android and FirefoxOS</a></h2>
    <p class="index-date">Jul 8th, 2013</p>
    <p><span class="tags"><a href="/tags/Android.html">Android</a>  <a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/FirefoxOS.html">FirefoxOS</a>  <a href="/tags/Mozilla.html">Mozilla</a>  <a href="/tags/Time.html">Time</a></span></p>
  </header>

<p>Today I did a quick port of Larry Doolittle&rsquo;s <a href="http://doolittle.icarus.com/ntpclient/">ntpclient program</a> to Android and FirefoxOS. Basically this lets you easily synchronize your device&rsquo;s time to that of a central server. Yes, there&rsquo;s lots and lots of Android &ldquo;applications&rdquo; which let you do this, but I wanted to be able to do this from the command line because that&rsquo;s how I roll. If you&rsquo;re interested, source and instructions are here:</p>

<p><a href="https://github.com/wlach/ntpclient-android">https://github.com/wlach/ntpclient-android</a></p>

<p>For those curious, no, I didn&rsquo;t just do this for fun. For next quarter, we want to write some Eideticker-based responsiveness tests for FirefoxOS and Android. For example, how long does it take from the time you tap on an icon in the homescreen on FirefoxOS to when the application is fully loaded? Or on Android, how long does it take to see a full list of sites in the awesomebar from the time you tap on the URL field and enter your search term?</p>

<p>Because an Eideticker test run involves two different machines (a host machine which controls the device and captures video of it in action, as well as the device itself), we need to use timestamps to really understand when and how events are being sent to the device. To do that reliably, we really need some easy way of synchronizing time between two machines (or at least accounting for the difference in their clocks, which amounts to about the same thing). NTP struck me as being the easiest, most standard way of doing this.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/05/proof-of-concept-eideticker-dashboard-for-firefoxos/">Proof of concept Eideticker dashboard for FirefoxOS</a></h2>
    <p class="index-date">May 6th, 2013</p>
    <p><span class="tags"><a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/FirefoxOS.html">FirefoxOS</a>  <a href="/tags/Mozilla.html">Mozilla</a></span></p>
  </header>

<p><em>[ For more information on the Eideticker software I&rsquo;m referring to, see <a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/">this entry</a> ]</em></p>

<p>I just put up a proof of concept Eideticker dashboard for FirefoxOS <a href="http://eideticker.wrla.ch/b2g">here</a>. Right now it has two days worth of data, manually sampled from an Unagi device running b2g18. Right now there are two tests: one the measures the &ldquo;speed&rdquo; of the contacts application scrolling, another that measures the amount of time it takes for the contacts application to be fully loaded.</p>

<p>For those not already familiar with it, Eideticker is a benchmarking suite which captures live video data coming from a device and analyzes it to determine performance. This lets us get data which is more representative of actual user experience (as opposed to an oft artificial benchmark). For example, Eideticker measures contacts startup as taking anywhere between 3.5 seconds and 4.5 seconds, versus than the 0.5 to 1 seconds that the <a href="https://datazilla.mozilla.org/b2g/?branch=master&amp;range=7&amp;test=cold_load_time&amp;app_list=contacts&amp;app=contacts&amp;gaia_rev=114bf216de0a19f7&amp;gecko_rev=9c0de2afd22a8476">existing datazilla benchmarks</a> show. What accounts for the difference? If you step through an eideticker-captured video, you can see that even though <em>something</em> appears very quickly, not all the contacts are displayed until the 3.5 second mark. There is a gap between an app being reported as &ldquo;loaded&rdquo; and it being fully available for use, which we had not been measuring until now.</p>

<p>At this point, I am most interested in hearing from FirefoxOS developers on new tests that would be interesting and useful to track performance of the system on an ongoing basis. I&rsquo;d obviously prefer to focus on things which have been difficult to measure accurately through other means. My setup is rather fiddly right now, but hopefully soon we can get some useful numbers going on an ongoing basis, as we <a href="http://eideticker.wrla.ch">do already</a> for Firefox for Android.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/04/actual-useful-firefoxos-eideticker-results-at-last/">Actual useful FirefoxOS Eideticker results at last</a></h2>
    <p class="index-date">Apr 22nd, 2013</p>
    <p><span class="tags"><a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/FirefoxOS.html">FirefoxOS</a>  <a href="/tags/Mozilla.html">Mozilla</a></span></p>
  </header>

<p>Another update on getting <a href="http://wrla.ch/blog/2013/02/eideticker-for-firefoxos/">Eideticker working with FirefoxOS</a>. Once again this is sort of high-level, looking forward to writing something more in-depth soon now that we have the basics working.</p>

<p>I finally got the last kinks out of the rig I was using to capture live video from FirefoxOS phones using the Point Grey devices last week. In order to make things reasonable I had to write some custom code to isolate the actual device screen from the rest of capture and a few other things. The setup looks interesting (reminds me a bit of something out of the War of the Worlds):</p>

<p><a href="/files/2013/04/eideticker-pointgrey-mounted.jpg"><img src="/files/2013/04/eideticker-pointgrey-mounted.jpg" alt="eideticker-pointgrey-mounted" width="512" height="683" class="alignnone size-full wp-image-894" srcset="/files/2013/04/eideticker-pointgrey-mounted-224x300.jpg 224w, /files/2013/04/eideticker-pointgrey-mounted.jpg 512w" sizes="(max-width: 512px) 100vw, 512px" /></a></p>

<p>Here&rsquo;s some example video of a test I wrote up to measure the performance of contacts scrolling performance (measured at a very respectable 44 frames per second, in case you wondering):</p>

<video src="/files/eideticker/contacts-scrolling-pointgrey.webm" controls="controls"></video>

<p>Surprisingly enough, I didn&rsquo;t wind up having to write up any code to compensate for a noisy image. Of course there&rsquo;s a certain amount of variance in every frame depending on how much light is hitting the camera sensor at any particular moment, but apparently not enough to interfere with getting useful results in the tests I&rsquo;ve been running.</p>

<p>Likely next step: Create some kind of chassis for mounting both the camera and device on a permanent basis (instead of an adhoc one on my desk) so we can start running these sorts of tests on a daily basis, much like we currently do with Android on the <a href="http://eideticker.wrla.ch">Eideticker Dashboard</a>.</p>

<p>As an aside, I&rsquo;ve been really impressed with both the <a href="https://wiki.mozilla.org/Auto-tools/Projects/Marionette">Marionette</a> framework and the gaiatests python module that was written up for FirefoxOS. Writing the above test took just 5 minutes &#8212; and <a href="https://github.com/mozilla/eideticker/blob/master/src/tests/b2g/appscrolling/scroll.py">the code</a> is quite straightforward. Quite the pleasant change from my various efforts in Android automation.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/03/eideticker-limitations-in-cross-browser-performance-testing/">Eideticker: Limitations in cross-browser performance testing</a></h2>
    <p class="index-date">Mar 20th, 2013</p>
    <p><span class="tags"><a href="/tags/Android.html">Android</a>  <a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/Mozilla.html">Mozilla</a></span></p>
  </header>

<p>Last summer I <a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/">wrote a bit</a> about using <a href="https://wiki.mozilla.org/Project_Eideticker">Eideticker</a> to measure the relative performance of Firefox for Android versus other browsers (Chrome, stock, etc.). At the time I was pretty optimistic about Eideticker&rsquo;s usefulness as a truly &ldquo;objective&rdquo; measure of user experience that would give us a more accurate view of how we compared against the competition than traditional benchmarking suites (which more often than not, measure things that a user will never see normally when browsing the web). Since then, there&rsquo;s been some things that I&rsquo;ve discovered, as well as some developments in terms of the &ldquo;state of the art&rdquo; in mobile browsing that have caused me to reconsider that view &#8212; while I haven&rsquo;t given up entirely on this concept (and I&rsquo;m still very much convinced of eideticker&rsquo;s utility as an internal benchmarking tool), there&rsquo;s definitely some limitations in terms of what we can do that I&rsquo;m not sure how to overcome.</p>

<p>Essentially, there are currently three different types of Eideticker performance tests:</p>

<ul>
 <li>Animation tests: Measure the smoothness of an animation by comparing frames and seeing how many are different. Currently the only example of this is the <a href="http://eideticker.wrla.ch/#/samsung-gn/clock/fps">canvas &ldquo;clock&rdquo; test</a>, but many others are possible.</li>
 <li>Startup tests: Measure the amount of time it takes from when the application is launched to when the browser is fully running/available. There are currently two variants of this test in the dashboard, both measure the amount of time taken to fully render Firefox&rsquo;s home screen (the only difference between the two is whether the browser profile is fully initialized). The <a href="http://eideticker.wrla.ch/#/samsung-gn/startup-abouthome-dirty/timetostableframe">dirty profile</a> benchmark probably most closely resembles what a user would usually experience.</li>
 <li>Scrolling tests: Measure the amount of undrawn areas when the user is panning a website. Most of the current eideticker tests are of this kind. A good example of this is the <a href="http://eideticker.wrla.ch/#/samsung-gn/taskjs/fps">taskjs benchmark</a>.</li></ul>

<p>In this blog post, I&rsquo;m going to focus on startup and scrolling tests. Animation tests are interesting, but they are also generally the sorts of tests that are easiest to measure in synthetic ways (e.g. by putting a frame counter in your javascript code) and have thus far not been a huge focus for Eideticker development.</p>

<p>As it turns out, it&rsquo;s unfortunately been rather difficult to create truly objective tests which measure the difference between browsers in these two categories. I&rsquo;ll go over them in order.</p>

<p><strong>Startup tests</strong></p>

<p>There are essentially two types of startup tests: one where you measure the amount of time to get to the browser&rsquo;s home screen when you explicitly launch the app (e.g. by pressing the Firefox icon in the app chooser), another where you load a web page in a browser from another app (e.g. by clicking on a link in the Twitter application).</p>

<p>The first is actually fairly easy to test across browsers, although we are not currently. There&rsquo;s not really a good reason for that, it was just an oversight, so I filed <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=852744">bug 852744</a> to add something like this.</p>

<p>The second case (startup to the browser&rsquo;s homescreen) is a bit more difficult. The problem here is that, in a nutshell, an apples to apples comparison is very difficult if not impossible simply because different browsers do different things when the user presses the application icon. Here&rsquo;s what we see with Firefox:</p>

<p><img src="/files/eideticker/firefox-startup.png" style="width:25%;" /></p>

<p>And here&rsquo;s what we see with Chrome:</p>

<p><img src="/files/eideticker/chrome-startup.png" style="width:25%;" /></p>

<p>And here&rsquo;s what we see with the stock browser:</p>

<p><img src="/files/eideticker/stock-startup.png" style="width:25%;" /></p>

<p>As you can see Chrome and the stock browser are totally different: they try to &ldquo;restore&rdquo; the browser back to its state from the last time (in Chrome&rsquo;s case, I was last visiting taskjs.org, in Stock&rsquo;s case, I was just on the homepage).</p>

<p>Personally I prefer Firefox&rsquo;s behaviour (generally I want to browse somewhere new when I press the icon on my phone), but that&rsquo;s really beside the point. It&rsquo;s possible to hack around what chrome is doing by restoring the profile between sessions to some sort of clean &ldquo;new tab&rdquo; state, but at that point you&rsquo;re not really reproducing a realistic user scenario. Sure, we can draw a comparison, but how valid is it really? It seems to me that the comparison is mostly only useful in a very broad &ldquo;how quickly does the user see something useful&rdquo; sense.</p>

<p><strong>Panning tests</strong></p>

<p>I had quite a bit of hope for these initially. They seemed like a place where Eideticker could do something that conventional benchmarking suites couldn&rsquo;t, as things like panning a web page are not presently possible to do in JavaScript. The main measure I tried to compare against was something called &ldquo;checkerboarding&rdquo;, which essentially represents the amount of time that the user waits for the page to redraw when panning around.</p>

<p>At the time that I wrote these tests, most browsers displayed regions that were not yet drawn while panning using the page background. We figured that it would thus be possible to detect regions of the page which were not yet drawn by looking for the background color while initiating a panning action. I thus hacked up existing web pages to have a magenta background, then wrote some image analysis code to detect regions that were that color (under the assumption that magenta is only rarely seen in webpages). It worked pretty well.</p>

<p>The world&rsquo;s moved on a bit since I wrote that: modern browsers like Chrome and Firefox use something like progressive drawing to display a lower resolution &ldquo;tile&rdquo; where possible in this case, so the user at least sees something resembling the actual page while panning on a slower device. To see what I mean, try visiting a slow-to-render site like taskjs.org and try panning down quickly. You should see something like this (click to expand):</p>

<p><a href="/files/eideticker/firefox-partialy-drawn.png"><img src="/files/eideticker/firefox-partialy-drawn.png" style="width:50%;" /></a></p>

<p>Unfortunately, while this is certainly a better user experience, it is not so easy to detect and measure. For Firefox, we&rsquo;ve disabled this behaviour so that we see the old checkerboard pattern. This is useful for our internal measurements (we can see both if our drawing code as well as our heuristics about when to draw are getting better or worse over time) but it only works for us.</p>

<p>If anyone has any suggestions on what to do here, let me know as I&rsquo;m a bit stuck. There are other metrics we could still compare against (i.e. how smooth is the panning animation aka frames per second?) but these aren&rsquo;t nearly as interesting.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/03/documentation-for-mozdevice/">Documentation for mozdevice</a></h2>
    <p class="index-date">Mar 11th, 2013</p>
    <p><span class="tags"><a href="/tags/Android.html">Android</a>  <a href="/tags/ateam.html">ateam</a>  <a href="/tags/FirefoxOS.html">FirefoxOS</a>  <a href="/tags/Mozilla.html">Mozilla</a></span></p>
  </header>

<p>Just wanted to give a quick heads up that as part of the ateam&rsquo;s ongoing effort to improve the documentation of our automated testing infrastructure, we now have <a href="https://mozbase.readthedocs.org/en/latest/mozdevice.html">online documentation</a> for mozdevice, the python library we use for interacting with Android- and FirefoxOS-based devices in automated testing.</p>

<p>Mozdevice is used in pretty much every one of our testing frameworks that has mobile support, including mochitest, reftest, <a href="https://wiki.mozilla.org/Buildbot/Talos">talos</a>, <a href="https://github.com/mozilla/autophone">autophone</a>, and <a href="https://wiki.mozilla.org/Project_Eideticker">eideticker</a>. Additionally, mozdevice is used by release engineering to clean up, monitor, and otherwise manage 
 <strike>our hundred-odd</strike> the 1200*** tegra and panda development boards that we use in <a href="http://tbpl.mozilla.org">tbpl</a>. See <a href="https://hg.mozilla.org/build/tools/file/tip/sut_tools">sut_tools</a> (old, buildbot-based, what we currently use) and <a href="https://github.com/mozilla/mozpool">mozpool</a> (the new and shiny future).</p>

<ul>
 <li>Thanks to Dustin Mitchell for the correction.</li></ul> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/03/follow-up-on-finding-a-camera-for-eideticker/">Follow up on &ldquo;Finding a Camera for Eideticker&rdquo;</a></h2>
    <p class="index-date">Mar 8th, 2013</p>
    <p><span class="tags"><a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/Mozilla.html">Mozilla</a></span></p>
  </header>

<p>Quick update on my <a href="http://wrla.ch/blog/2013/02/finding-a-camera-for-eideticker/">last post</a> about finding some kind of camera suitable for use in automated performance testing of fennec and b2g with eideticker. Shortly after I wrote that, I found out about a company called <a href="http://ptgrey.com">Point Grey Research</a> which manufactures custom web cameras intended for exactly the sorts of things we&rsquo;re doing. Initial results with the <a href="http://ww2.ptgrey.com/USB3/Flea3">Flea3 camera</a> I ordered from them are quite promising:</p>

<video width="400px" src="/files/eideticker/pointgrey-taskjs.webm" controls="controls"></video>

<p>(the actual capture is even higher quality than that would suggest some detail is lost in the conversion to webm)</p>

<p>There seems to be some sort of issue with the white balance in that capture which is causing a flashing effect (I suspect this just involves flipping some kind of software setting: still trying to grok their SDK), and we&rsquo;ll need to create some sort of enclosure for the setup so ambient light doesn&rsquo;t interfere with the capture but overall I&rsquo;m pretty optimistic about this baby. 60 frames per second, very high resolution (1280x960), no issues with HDMI (since it&rsquo;s just a USB camera), relatively inexpensive.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/02/finding-a-camera-for-eideticker/">Finding a camera for Eideticker</a></h2>
    <p class="index-date">Feb 19th, 2013</p>
    <p><span class="tags"><a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/FirefoxOS.html">FirefoxOS</a>  <a href="/tags/Mozilla.html">Mozilla</a></span></p>
  </header>

<p><em>[ For more information on the Eideticker software I&rsquo;m referring to, see <a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/">this entry</a> ]</em></p>

<p>Ok, so as I mentioned <a href="http://wrla.ch/blog/2013/02/eideticker-for-firefoxos/">last time</a> I&rsquo;ve been looking into making Eideticker work for devices without native HDMI output by capturing their output with some kind of camera. So far I&rsquo;ve tried four different DSLRs for this task, which have all been inadequate for different reasons. I was originally just going to write an email about this to a few concerned parties, but then figured I may as well structure it into a blog post. Maybe someone will find it useful (or better yet, have some ideas.)</p>

<p><strong>Elmo MO&ndash;1</strong></p>

<p>This is the device I mentioned last time. Easy to set up, plays nicely with the Decklink capture card we&rsquo;re using for Eideticker. It seemed almost perfect, except for that:</p>

<ol>
 <li>The image quality is <em>really</em> bad (beaten even by $200 standard digital camera). Tons of noise, picture quality really bad. Not *necessarily* a deal breaker, but it still sucks.</li>
 <li>More importantly, there seems to be no way of turning off the auto white balance adjustment. This makes automated image analysis impossible if the picture changes, as is highlighted in this video:  
  <video width="400px" src="/files/eideticker/elmo-white-balance-problem.webm" controls="controls"></video></li></ol>

<p><strong>Canon Rebel T4i</strong></p>

<p>This is the first camera that was recommended to me at the camera shop I&rsquo;ve been going to. It does have an HDMI output signal, but it&rsquo;s not &ldquo;clean&rdquo;. This <a href="http://www.hireacamera.com/blog/index.asp?post=canon-eos-650d--hdmi-explained">blog post</a> explains the details better than I could. Next.</p>

<p><strong>Nikon D600</strong></p>

<p>Supposedly does native clean 720p output, but unfortunately the <a href="http://vimeo.com/49952287">output is in a &ldquo;box&rdquo;</a> so isn&rsquo;t recognized by the Decklink cards that we&rsquo;re using (which insist on a full 1280&#215;720 HDMI signal to work). Next.</p>

<p><strong>Nikon D800</strong></p>

<p>It is possible to configure this one to not put a box around the output, so the Decklink card does recognize it. Except that the camera shuts off the HDMI signal whenever the input parameters change on the card or the signal input is turned on, which essentially makes it useless for Eideticker (this happens every time we start the Eideticker harness). Quite a shame, as the HDMI signal is quite nice otherwise.</p>

<p>&#8212;</p>

<p>To be clear, with the exception of the Elmo all the devices above seem like fine cameras, and should more than do for manual captures of B2G or Android phones (which is something we probably want to do anyway). But for Eideticker, we need something that works in automation, and none of the above fit the bill. I guess I could explore using a &ldquo;real&rdquo; video camera as opposed to a DSLR acting like one, though I suspect I might run into some of the same sorts of issues depending on how the HDMI output of those devices behaves.</p>

<p>Part of me wonders whether a custom solution wouldn&rsquo;t work better. How complicated could it be to construct your own digital camera anyway? ðŸ˜‰ Hook up a fancy camera sensor to a <a href="http://pandaboard.org">pandaboard</a>, get it to output through the HDMI port, and then we&rsquo;re set? Or better yet, maybe just get a fancy webcam like the <a href="http://en.wikipedia.org/wiki/PlayStation_Eye">Playstation Eye</a> and hook it up directly to a computer? That would eliminate the need for our expensive video capture card setup altogether.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/02/eideticker-for-firefoxos/">Eideticker for FirefoxOS</a></h2>
    <p class="index-date">Feb 1st, 2013</p>
    <p><span class="tags"><a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/FirefoxOS.html">FirefoxOS</a>  <a href="/tags/Mozilla.html">Mozilla</a></span></p>
  </header>

<p><em>[ For more information on the Eideticker software I&rsquo;m referring to, see <a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/">this entry</a> ]</em></p>

<p>Here&rsquo;s a long overdue update on where we&rsquo;re at with Eideticker for FirefoxOS. While we&rsquo;ve had a good amount of success getting <a href="http://eideticker.wrla.ch">useful, actionable data</a> out of Eideticker for Android, so far we haven&rsquo;t been able to replicate that success for FirefoxOS. This is not for lack of trying: first, <a href="http://nakubu.com/">Malini Das</a> and then me have been working at it since summer 2012.</p>

<p>When it comes right down to it, instrumenting Eideticker for B2G is just a whole lot more complex. On Android, we could take the operating system (including support for all the things we needed, like HDMI capture) as a given. The only tricky part was instrumenting the capture so the right things happened at the right moment. With FirefoxOS, we need to run these tests on entire builds of a whole operating system which was constantly changing. Not nearly as simple. That said, I&rsquo;m starting to see light at the end of the tunnel.</p>

<p><strong>Platforms</strong></p>

<p>We initially selected the <a href="http://pandaboard.org">pandaboard</a> as the main device to use for eideticker testing, for two reasons. First, it&rsquo;s the same hardware platform we&rsquo;re targeting for other b2g testing in tbpl (mochitest, reftest, etc.), and is the platform we&rsquo;re using for running Gaia UI tests. Second, unlike every other device that we&rsquo;re prototyping FirefoxOS on (to my knowledge), it has HDMI-out capability, so we can directly interface it with the Eideticker video capture setup.</p>

<p>However, the panda also has some serious shortcomings. First, it&rsquo;s obviously not a platform we&rsquo;re shipping, so the performance we&rsquo;re seeing from it is subject to different factors that we might not see with a phone actually shipped to users. For the same reason, we&rsquo;ve had many problems getting B2G running reliably on it, as it&rsquo;s not something most developers have been hacking on a day to day basis. Thanks to the heroic efforts of Thomas Zimmerman, we&rsquo;ve mostly got things working ok now, but it was a fairly long road to get here (several months last fall).</p>

<p>More recently, we became aware of something called an <a href="http://www.elmousa.com/">Elmo</a> which might let us combine
 <br />the best of both worlds. An elmo is really just a tiny mounted video camera with a bunch of outputs, and is I understand most commonly used to project documents in a classroom/presentation setting. However, it seems to do a great job of capturing mobile phones in action as well.</p>

<video width="400px" src="/files/eideticker/startup-test-elmo.webm" controls="controls"></video>

<p>The nice thing about using an external camera for the video capture part of eideticker is that we are no longer limited to devices with HDMI out &#8212; we can run the standard set of automated tests on ANYTHING. We&rsquo;ve already used this to some success in getting some videos of FirefoxOS startup times versus Android on the Unagi (a development phone that we&rsquo;re using internally) for manual analysis. Automating this process may be trickier because of the fact that the video capture is no longer &ldquo;perfect&rdquo;, but we may be able to work around that (more discussion about this later).</p>

<p><strong>FirefoxOS web page tests</strong></p>

<p>These are the same tests we run on Android. They should give us an idea of roughly where our performance when browsing / panning web sites like CNN. So far, I&rsquo;ve only run these tests on the Pandaboard and they are INCREDIBLY slow (like 1&ndash;3 frames per second when scrolling). So much so that I have to think there is something broken about our hardware acceleration on this platform.</p>

<p><strong>FirefoxOS application tests</strong></p>

<p>These are some new tests written in a framework that allows you to script arbitrary interactions in FirefoxOS, like launching applications or opening the task switcher.</p>

<p>I&rsquo;m pretty happy with this. It seems to work well. The only problems I&rsquo;m seeing with this is with the platform we&rsquo;re running these tests on. With the pandaboard, applications look weird (since the screen resolution doesn&rsquo;t remotely resemble the 320&#215;480 resolution on our current devices) and performance is abysmal. Take, for example, this capture of application switching performance, which operates only at roughly 3&ndash;4 fps:</p>

<video width="400px" src="/files/eideticker/b2g-appswitching-video.webm" controls="controls"></video>

<p><strong>So what now?</strong></p>

<p>I&rsquo;m not 100% sure yet (partly it will depend on what others say as well as my own investigation), but I have a feeling that capturing video of real devices running FirefoxOS using the Elmo is the way forward. First, the hardware and driver situation will be much more representative of what we&rsquo;ll actually be shipping to users. Second, we can flash new builds of FirefoxOS onto them automatically, unlike the pandaboards where you currently either need to manually flash and reset (a time consuming and error prone process) or set up an instance of <a href="https://github.com/djmitche/mozpool">mozpool</a> (which I understand is quite complicated).</p>

<p>The main use case I see with eideticker-on-panda would be where we wanted to run a suite of tests on checkin (in tbpl-like fashion) and we&rsquo;d need to scale to many devices. While cool, this sounds like an expensive project (both in terms of time and hardware) and I think we&rsquo;d do better with getting something slightly smaller-scale running first.</p>

<p>So, the real question is whether or not the capture produced by the Elmo is amenable to the same analysis that we do on the raw HDMI output. At the very least, some of eideticker&rsquo;s image analysis code will have to be adapted to handle a much &ldquo;noisier&rdquo; capture. As opposed to capturing the raw HDMI signal, we now have to deal with the real world and its irritating fluctuations in ambient light levels and all that the rest. I have no doubt it is *possible* to compensate for this (after all this is what the human eye/brain does all the time), but the question is how much work it will be. Can&rsquo;t speak for anyone else at Mozilla, but I&rsquo;m not sure if I really have the time to start a Ph.D-level research project in computational vision. ðŸ˜‰ I&rsquo;m optimistic that won&rsquo;t be necessary, but we&rsquo;ll just have to wait and see.</p> 
  <hr/>
</article>
<footer>
 <ul class="pagination">
  <li class="page-item"><a class="page-link" href="/tags/Mozilla-7.html">
    <quote>&larr;</quote></a></li>
  <li class="page-item"><a class="page-link" href="/tags/Mozilla.html">1</a></li>
  <li class="page-item"><a class="page-link" href="/tags/Mozilla-2.html">2</a></li>
  <li class="page-item"><a class="page-link" href="/tags/Mozilla-3.html">3</a></li>
  <li class="page-item"><a class="page-link" href="/tags/Mozilla-4.html">4</a></li>
  <li class="page-item"><a class="page-link" href="/tags/Mozilla-5.html">5</a></li>
  <li class="page-item"><a class="page-link" href="/tags/Mozilla-6.html">6</a></li>
  <li class="page-item"><a class="page-link" href="/tags/Mozilla-7.html">7</a></li>
  <li class="page-item active"><a class="page-link" href="/tags/Mozilla-8.html">8</a></li>
  <li class="page-item"><a class="page-link" href="/tags/Mozilla-9.html">9</a></li>
  <li class="page-item"><a class="page-link" href="/tags/Mozilla-10.html">10</a></li>
  <li class="page-item"><a class="page-link" href="/tags/Mozilla-11.html">11</a></li>
  <li class="page-item"><a class="page-link" href="/tags/Mozilla-9.html">
    <quote>&rarr;</quote></a></li></ul></footer>
    </div>
    <footer class="container max-w-screen-md px-8 py-4 mx-auto less-important">
      <p>Comments / thoughts? Feel free to send an email to wlach on protonmail.com or
        (for Mozilla-related stuff) reach me at <code>wlach</code> on <a href="https://wiki.mozilla.org/Matrix">Mozilla's instance of Matrix</a>.</p>
      <p>
        Site generated by
        <a href="https://github.com/greghendershott/frog">Frog</a>.
        Post content is licensed under a
        <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"
          >Creative Commons Attribution 4.0 Unported License</a
        >.
      </p>
    </footer>
  </body>
</html>